{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up an Anaconda environment for implementing the Transformer model in PyTorch, follow these steps:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Create a New Conda Environment**\n",
    "Open a terminal and run:\n",
    "```bash\n",
    "conda create --name attention-is-all-you-need python=3.12\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Activate the Environment**\n",
    "```bash\n",
    "conda activate attention-is-all-you-need\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Install PyTorch**\n",
    "For GPU (CUDA):\n",
    "```bash\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "```\n",
    "For CPU (if you donâ€™t have a compatible GPU):\n",
    "```bash\n",
    "conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
    "```\n",
    "Check if PyTorch is installed correctly:\n",
    "```python\n",
    "python -c \"import torch; print(torch.__version__)\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Install Essential Libraries**\n",
    "```bash\n",
    "pip install numpy pandas matplotlib tqdm\n",
    "```\n",
    "- `numpy`: Tensor operations\n",
    "- `pandas`: Data handling (optional, useful for datasets)\n",
    "- `matplotlib`: Visualization\n",
    "- `tqdm`: Progress bars for training\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Install NLP Libraries (If Needed)**\n",
    "```bash\n",
    "pip install transformers datasets tokenizers sentencepiece\n",
    "```\n",
    "- `transformers`: Pretrained models from Hugging Face (optional)\n",
    "- `datasets`: NLP datasets from Hugging Face\n",
    "- `tokenizers`: Efficient tokenization\n",
    "- `sentencepiece`: Subword tokenization (used in original Transformer)\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Install Jupyter Notebook (Optional)**\n",
    "If you want to develop in Jupyter:\n",
    "```bash\n",
    "conda install jupyter\n",
    "```\n",
    "Then start Jupyter:\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Verify Everything**\n",
    "Run the following to ensure your environment is properly set up:\n",
    "```python\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Save the Environment (Optional)**\n",
    "To export your environment for reproducibility:\n",
    "```bash\n",
    "conda env export > environment.yml\n",
    "```\n",
    "To recreate it later:\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int):\n",
    "        \"\"\"\n",
    "        Initializes the embedding layer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Number of unique tokens in the vocabulary.\n",
    "            d_model (int): Dimension of the embedding vectors.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Define the embedding layer that maps token indices to dense vectors.\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)  \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for token embedding.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor of shape (batch_size, seq_len) containing token indices.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of shape (batch_size, seq_len, d_model) containing embedded representations.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the lookup operation using the embedding layer.\n",
    "        embedded = self.embedding(x)  \n",
    "\n",
    "        return embedded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def run_tests():\n",
    "    # Test Parameters\n",
    "    vocab_size = 100\n",
    "    d_model = 16\n",
    "    batch_size = 4\n",
    "    seq_len = 10\n",
    "\n",
    "    # Create a sample input tensor\n",
    "    test_input = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "    # Initialize TokenEmbedding\n",
    "    embedding_layer = TokenEmbedding(vocab_size, d_model)\n",
    "\n",
    "    # Test 1: Check Output Shape\n",
    "    output = embedding_layer(test_input)\n",
    "    assert output.shape == (batch_size, seq_len, d_model), f\"Unexpected shape: {output.shape}\"\n",
    "    \n",
    "    # Test 2: Ensure Output is a Tensor of Correct Type\n",
    "    assert isinstance(output, torch.Tensor), \"Output is not a tensor\"\n",
    "    assert output.dtype == torch.float32, f\"Unexpected dtype: {output.dtype}\"\n",
    "    \n",
    "    # Test 3: Check if the Same Token Index Maps to the Same Embedding\n",
    "    index = torch.tensor([[5]])\n",
    "    embedding_1 = embedding_layer(index)\n",
    "    embedding_2 = embedding_layer(index)\n",
    "    assert torch.allclose(embedding_1, embedding_2), \"Embeddings should be identical for the same index\"\n",
    "    \n",
    "    # Test 4: Check if Different Indices Give Different Embeddings\n",
    "    index1 = torch.tensor([[5]])\n",
    "    index2 = torch.tensor([[8]])\n",
    "    embedding_1 = embedding_layer(index1)\n",
    "    embedding_2 = embedding_layer(index2)\n",
    "    assert not torch.allclose(embedding_1, embedding_2), \"Different indices should have different embeddings\"\n",
    "    \n",
    "    # Test 5: Check if Gradients are Computed\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    assert embedding_layer.embedding.weight.grad is not None, \"Gradients should not be None\"\n",
    "    assert embedding_layer.embedding.weight.grad.shape == (vocab_size, d_model), \"Gradient shape mismatch\"\n",
    "    \n",
    "    print(\"âœ… All tests passed successfully!\")\n",
    "\n",
    "# Run all tests\n",
    "run_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.7030,  0.0881, -0.1770], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = TokenEmbedding(vocab_size=10, d_model=3)\n",
    "embedding_layer(torch.tensor(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        \"\"\"\n",
    "        Initializes positional encoding.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimension of the embedding vectors.\n",
    "            max_len (int): Maximum sequence length.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Create an empty tensor to hold positional encodings of shape (max_len, d_model)\n",
    "        pe = torch.zeros(size=(max_len, d_model))\n",
    "\n",
    "        # TODO: Create a position index tensor of shape (max_len, 1)\n",
    "        positions = torch.arange(max_len).unsqueeze(1)  # Replace with the correct initialization\n",
    "\n",
    "        # TODO: Compute the denominator term for the sine/cosine functions\n",
    "        div_term = 10**4**(2*positions/d_model)  # Replace with the correct computation\n",
    "\n",
    "        # TODO: Compute sin and cos positional encodings\n",
    "        # Apply sine to even indices and cosine to odd indices\n",
    "        # Hint: Use slicing `self.pe[:, 0::2] = ...` for even indices\n",
    "        #       Use slicing `self.pe[:, 1::2] = ...` for odd indices\n",
    "        pe[:, 0::2] = torch.sin(positions/div_term)\n",
    "        pe[:, 1::2] = torch.cos(positions/div_term)\n",
    "\n",
    "        # TODO: Register `self.pe` as a buffer so it doesn't update during training\n",
    "        # Hint: Use `self.register_buffer(\"pe\", self.pe)`\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Adds positional encoding to the input embeddings.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor of shape (batch_size, seq_len, d_model) containing input embeddings.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of shape (batch_size, seq_len, d_model) with positional encodings added.\n",
    "        \"\"\"\n",
    "        # TODO: Retrieve only the necessary positions up to the input sequence length\n",
    "        # Hint: Slice `self.pe` correctly based on `x.size(1)`\n",
    "        pe_slice = self.pe[:x.size(1),:].unsqueeze(0)\n",
    "\n",
    "        # TODO: Add positional encodings to the input embeddings\n",
    "        # Hint: Ensure the positional encodings are on the same device as `x`\n",
    "        pe_slice.to(x.device)\n",
    "\n",
    "        return x + pe_slice  # Replace with the final tensor with positional encoding added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All positional encoding tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def run_positional_encoding_tests():\n",
    "    d_model = 16\n",
    "    seq_len = 10\n",
    "    batch_size = 4\n",
    "\n",
    "    test_input = torch.zeros((batch_size, seq_len, d_model))  # Placeholder embeddings\n",
    "    pos_encoding = PositionalEncoding(d_model=d_model)\n",
    "\n",
    "    # Test 1: Check Output Shape\n",
    "    output = pos_encoding(test_input)\n",
    "    assert output.shape == (batch_size, seq_len, d_model), f\"Unexpected shape: {output.shape}\"\n",
    "    \n",
    "    # Test 2: Ensure Output is a Tensor of Correct Type\n",
    "    assert isinstance(output, torch.Tensor), \"Output is not a tensor\"\n",
    "    assert output.dtype == torch.float32, f\"Unexpected dtype: {output.dtype}\"\n",
    "    \n",
    "    # Test 3: Check if Positional Encoding is Being Added\n",
    "    assert not torch.allclose(test_input, output), \"Positional encoding is not being added!\"\n",
    "    \n",
    "    # Test 4: Check Device Compatibility\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    test_input = test_input.to(device)\n",
    "    pos_encoding = pos_encoding.to(device)\n",
    "    output = pos_encoding(test_input)\n",
    "    assert output.device == test_input.device, f\"Device mismatch: {output.device} vs {test_input.device}\"\n",
    "    \n",
    "    # Test 5: Check if Encodings are Deterministic\n",
    "    output1 = pos_encoding(test_input)\n",
    "    output2 = pos_encoding(test_input)\n",
    "    assert torch.allclose(output1, output2), \"Positional encoding should be deterministic!\"\n",
    "    \n",
    "    print(\"âœ… All positional encoding tests passed successfully!\")\n",
    "\n",
    "# Run all tests\n",
    "run_positional_encoding_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+01],\n",
      "        [2.0869e+01],\n",
      "        [5.5094e+01],\n",
      "        [1.9833e+02],\n",
      "        [1.0751e+03],\n",
      "        [1.0000e+04],\n",
      "        [1.8968e+05],\n",
      "        [9.2131e+06],\n",
      "        [1.5473e+09],\n",
      "        [1.3358e+12],\n",
      "        [1.0000e+16],\n",
      "        [1.2946e+21],\n",
      "        [7.2048e+27]])\n"
     ]
    }
   ],
   "source": [
    "def scratchboard(max_len, d_model):\n",
    "    pe = torch.zeros(size=(max_len, d_model))\n",
    "    positions = torch.arange(max_len).unsqueeze(1)\n",
    "    div_term = 10**4**(2*positions/d_model)\n",
    "    print(div_term)\n",
    "\n",
    "scratchboard(13, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k: int):\n",
    "        \"\"\"\n",
    "        Initializes scaled dot-product attention.\n",
    "\n",
    "        Args:\n",
    "            d_k (int): Dimension of the key vectors (used for scaling).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Store d_k for scaling attention scores\n",
    "        self.d_k = d_k  # Replace with correct initialization\n",
    "\n",
    "    def forward(\n",
    "        self, \n",
    "        query: torch.Tensor, \n",
    "        key: torch.Tensor, \n",
    "        value: torch.Tensor, \n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Computes the scaled dot-product attention.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): Shape (batch_size, num_heads, seq_len, d_k)\n",
    "            key (torch.Tensor): Shape (batch_size, num_heads, seq_len, d_k)\n",
    "            value (torch.Tensor): Shape (batch_size, num_heads, seq_len, d_v)\n",
    "            mask (Optional[torch.Tensor]): Shape (batch_size, 1, seq_len, seq_len) \n",
    "                                           (mask for padding or future tokens in decoder)\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: \n",
    "                - Attention output of shape (batch_size, num_heads, seq_len, d_v)\n",
    "                - Attention weights of shape (batch_size, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        # TODO: Compute attention scores as QK^T / sqrt(d_k)\n",
    "        attention_scores = torch.matmul(query, torch.transpose(key, -2, -1)) / math.sqrt(self.d_k)   # Replace with correct computation\n",
    "\n",
    "        # TODO: Apply mask (if provided) by setting masked positions to a very low value\n",
    "        # Hint: Use `float('-inf')` for masked positions before applying softmax\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill_(mask.bool(), float('-inf'))\n",
    "\n",
    "        # TODO: Compute attention weights using softmax\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)  # Replace with correct computation\n",
    "\n",
    "        # TODO: Multiply attention weights by value matrix to get the final output\n",
    "        output = torch.matmul(attention_weights, value)  # Replace with correct computation\n",
    "\n",
    "        return output, attention_weights  # Return attention output and weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Output shape test passed!\n",
      "âœ… Attention weights shape test passed!\n",
      "âœ… Softmax test passed!\n",
      "âœ… Deterministic output test passed!\n",
      "âœ… Masking test passed!\n",
      "ðŸŽ‰ All ScaledDotProductAttention tests passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def test_scaled_dot_product_attention():\n",
    "    \"\"\"\n",
    "    Tests the ScaledDotProductAttention module for correctness.\n",
    "    \"\"\"\n",
    "    batch_size = 2\n",
    "    num_heads = 4\n",
    "    seq_len = 5\n",
    "    d_k = 8\n",
    "    d_v = 8\n",
    "\n",
    "    # Initialize test input tensors\n",
    "    query = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "    key = torch.randn(batch_size, num_heads, seq_len, d_k)\n",
    "    value = torch.randn(batch_size, num_heads, seq_len, d_v)\n",
    "\n",
    "    # Initialize attention module\n",
    "    attention = ScaledDotProductAttention(d_k)\n",
    "\n",
    "    # Run forward pass without a mask\n",
    "    output, attention_weights = attention(query, key, value, mask=None)\n",
    "\n",
    "    # Test 1: Check output shape\n",
    "    assert output.shape == (batch_size, num_heads, seq_len, d_v), \\\n",
    "        f\"Unexpected output shape: {output.shape}\"\n",
    "    print(\"âœ… Output shape test passed!\")\n",
    "\n",
    "    # Test 2: Check attention weights shape\n",
    "    assert attention_weights.shape == (batch_size, num_heads, seq_len, seq_len), \\\n",
    "        f\"Unexpected attention weights shape: {attention_weights.shape}\"\n",
    "    print(\"âœ… Attention weights shape test passed!\")\n",
    "\n",
    "    # Test 3: Check softmax behavior (sum of attention weights should be ~1 per row)\n",
    "    sum_weights = attention_weights.sum(dim=-1)  # Sum over last dim\n",
    "    assert torch.allclose(sum_weights, torch.ones_like(sum_weights), atol=1e-5), \\\n",
    "        \"Softmax output does not sum to 1\"\n",
    "    print(\"âœ… Softmax test passed!\")\n",
    "\n",
    "    # Test 4: Check deterministic output for same input\n",
    "    output_2, attention_weights_2 = attention(query, key, value, mask=None)\n",
    "    assert torch.allclose(output, output_2), \"Output should be deterministic!\"\n",
    "    assert torch.allclose(attention_weights, attention_weights_2), \"Attention weights should be deterministic!\"\n",
    "    print(\"âœ… Deterministic output test passed!\")\n",
    "\n",
    "    # Test 5: Apply a mask and check if it works\n",
    "    mask = torch.zeros(batch_size, 1, seq_len, seq_len)\n",
    "    mask[:, :, :, -1] = float('-inf')  # Mask the last token\n",
    "\n",
    "    output_masked, attention_weights_masked = attention(query, key, value, mask=mask)\n",
    "\n",
    "    # The last column of attention weights should be very small (close to 0)\n",
    "    assert torch.all(attention_weights_masked[:, :, :, -1] < 1e-3), \\\n",
    "        \"Masking is not applied correctly!\"\n",
    "    print(\"âœ… Masking test passed!\")\n",
    "\n",
    "    print(\"ðŸŽ‰ All ScaledDotProductAttention tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_scaled_dot_product_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        \"\"\"\n",
    "        Initializes multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimension of the model (input and output size).\n",
    "            num_heads (int): Number of attention heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Ensure d_model is divisible by num_heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "\n",
    "        # TODO: Define linear transformations for query, key, and value\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # Replace with nn.Linear\n",
    "        self.W_k = nn.Linear(d_model, d_model)  # Replace with nn.Linear\n",
    "        self.W_v = nn.Linear(d_model, d_model)  # Replace with nn.Linear\n",
    "\n",
    "        # TODO: Define output projection layer\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # Replace with nn.Linear\n",
    "\n",
    "        # TODO: Define the scaled dot-product attention module\n",
    "        self.attention = ScaledDotProductAttention(self.d_k)  # Replace with ScaledDotProductAttention(self.d_k)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): Shape (batch_size, seq_len, d_model)\n",
    "            key (torch.Tensor): Shape (batch_size, seq_len, d_model)\n",
    "            value (torch.Tensor): Shape (batch_size, seq_len, d_model)\n",
    "            mask (Optional[torch.Tensor]): Shape (batch_size, 1, seq_len, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Shape (batch_size, seq_len, d_model) - Multi-head attention output.\n",
    "        \"\"\"\n",
    "        # TODO: Apply linear transformations to query, key, and value\n",
    "        Q = self.W_q(query)  # Replace with correct transformation\n",
    "        K = self.W_k(key)  # Replace with correct transformation\n",
    "        V = self.W_v(value)  # Replace with correct transformation\n",
    "\n",
    "        # TODO: Reshape Q, K, V for multi-head attention\n",
    "        # Hint: Use `.view()` and `.transpose()` to shape into (batch_size, num_heads, seq_len, d_k)\n",
    "        batch_size, seq_len, _ = query.shape\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # TODO: Apply scaled dot-product attention\n",
    "        output, attention_weights = self.attention(Q, K, V, mask)  # Replace with correct computation\n",
    "\n",
    "        # TODO: Concatenate the heads back and apply final linear transformation\n",
    "        # Current shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        # We first swap num_heads and seq_len\n",
    "        output = output.transpose(1, 2)  # (batch_size, seq_len, num_heads, d_k)\n",
    "        output = output.contiguous().view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        output = self.W_o(output)  # Replace with correct transformation\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Output shape test passed!\n",
      "âœ… Tensor type test passed!\n",
      "âœ… Deterministic output test passed!\n",
      "âœ… Masking test passed!\n",
      "ðŸŽ‰ All MultiHeadAttention tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_multi_head_attention():\n",
    "    \"\"\"\n",
    "    Tests the MultiHeadAttention module for correctness.\n",
    "    \"\"\"\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "    d_model = 16\n",
    "    num_heads = 4\n",
    "\n",
    "    # Initialize test input tensors\n",
    "    query = torch.randn(batch_size, seq_len, d_model)\n",
    "    key = torch.randn(batch_size, seq_len, d_model)\n",
    "    value = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # Initialize multi-head attention module\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    # Run forward pass without a mask\n",
    "    output = mha(query, key, value, mask=None)\n",
    "\n",
    "    # Test 1: Check output shape\n",
    "    assert output.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Unexpected output shape: {output.shape}\"\n",
    "    print(\"âœ… Output shape test passed!\")\n",
    "\n",
    "    # Test 2: Ensure output is a tensor\n",
    "    assert isinstance(output, torch.Tensor), \"Output is not a tensor\"\n",
    "    print(\"âœ… Tensor type test passed!\")\n",
    "\n",
    "    # Test 3: Check deterministic output for same input\n",
    "    output_2 = mha(query, key, value, mask=None)\n",
    "    assert torch.allclose(output, output_2), \"Output should be deterministic!\"\n",
    "    print(\"âœ… Deterministic output test passed!\")\n",
    "\n",
    "    # Test 4: Apply a mask and check if masking works\n",
    "    mask = torch.zeros(batch_size, 1, seq_len, seq_len)\n",
    "    mask[:, :, :, -1] = float('-inf')  # Mask the last token\n",
    "\n",
    "    output_masked = mha(query, key, value, mask=mask)\n",
    "\n",
    "    # Ensure output is still the correct shape\n",
    "    assert output_masked.shape == (batch_size, seq_len, d_model), \\\n",
    "        \"Masked output shape mismatch\"\n",
    "    print(\"âœ… Masking test passed!\")\n",
    "\n",
    "    print(\"ðŸŽ‰ All MultiHeadAttention tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_multi_head_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model) - Transformed representations.\n",
    "        \"\"\"\n",
    "        output = self.fc1(x)\n",
    "        output = self.relu(output)\n",
    "        output = self.fc2(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Output shape test passed!\n",
      "âœ… Tensor type test passed!\n",
      "âœ… ReLU activation test passed!\n",
      "âœ… Deterministic output test passed!\n",
      "âœ… Gradient computation test passed!\n",
      "ðŸŽ‰ All PositionwiseFeedForward tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_positionwise_feedforward():\n",
    "    \"\"\"\n",
    "    Tests the PositionwiseFeedForward module.\n",
    "    \"\"\"\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "    d_model = 16\n",
    "    d_ff = 32  # Expanded dimension\n",
    "\n",
    "    # Initialize test input tensor (random)\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # Initialize the feed-forward module\n",
    "    ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "\n",
    "    # Run forward pass\n",
    "    output = ffn(x)\n",
    "\n",
    "    # Test 1: Check output shape\n",
    "    assert output.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Unexpected output shape: {output.shape}\"\n",
    "    print(\"âœ… Output shape test passed!\")\n",
    "\n",
    "    # Test 2: Ensure output is a tensor\n",
    "    assert isinstance(output, torch.Tensor), \"Output is not a tensor\"\n",
    "    print(\"âœ… Tensor type test passed!\")\n",
    "\n",
    "    # Test 3: Ensure ReLU activation is applied\n",
    "    hidden_layer_output = ffn.fc1(x)  # Get pre-ReLU values\n",
    "    assert torch.all((hidden_layer_output > 0) == (ffn.relu(hidden_layer_output) > 0)), \\\n",
    "        \"ReLU activation is not applied correctly\"\n",
    "    print(\"âœ… ReLU activation test passed!\")\n",
    "\n",
    "    # Test 4: Check deterministic output for same input\n",
    "    output_2 = ffn(x)\n",
    "    assert torch.allclose(output, output_2), \"Output should be deterministic!\"\n",
    "    print(\"âœ… Deterministic output test passed!\")\n",
    "\n",
    "    # Test 5: Check gradients (ensuring backpropagation works)\n",
    "    output.sum().backward()  # Compute gradients\n",
    "    assert ffn.fc1.weight.grad is not None, \"Gradients are not computed for fc1!\"\n",
    "    assert ffn.fc2.weight.grad is not None, \"Gradients are not computed for fc2!\"\n",
    "    print(\"âœ… Gradient computation test passed!\")\n",
    "\n",
    "    print(\"ðŸŽ‰ All PositionwiseFeedForward tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_positionwise_feedforward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model) - Input to encoder layer.\n",
    "            mask: (batch_size, 1, seq_len, seq_len) - Optional attention mask.\n",
    "\n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model) - Encoder layer output.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, memory: torch.Tensor, \n",
    "                src_mask: Optional[torch.Tensor] = None, \n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model) - Input to decoder layer.\n",
    "            memory: (batch_size, seq_len, d_model) - Encoder output.\n",
    "            src_mask: (batch_size, 1, seq_len, seq_len) - Optional encoder mask.\n",
    "            tgt_mask: (batch_size, 1, seq_len, seq_len) - Optional decoder mask.\n",
    "\n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model) - Decoder layer output.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model) - Input sequence.\n",
    "            mask: (batch_size, 1, seq_len, seq_len) - Optional mask.\n",
    "\n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model) - Encoder output.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, memory: torch.Tensor, \n",
    "                src_mask: Optional[torch.Tensor] = None, \n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model) - Target sequence.\n",
    "            memory: (batch_size, seq_len, d_model) - Encoder output.\n",
    "            src_mask: (batch_size, 1, seq_len, seq_len) - Optional encoder mask.\n",
    "            tgt_mask: (batch_size, 1, seq_len, seq_len) - Optional decoder mask.\n",
    "\n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model) - Decoder output.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, num_layers: int, num_heads: int, \n",
    "                 d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, \n",
    "                src_mask: Optional[torch.Tensor] = None, \n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: (batch_size, src_seq_len) - Source token indices.\n",
    "            tgt: (batch_size, tgt_seq_len) - Target token indices.\n",
    "            src_mask: (batch_size, 1, src_seq_len, src_seq_len) - Optional source mask.\n",
    "            tgt_mask: (batch_size, 1, tgt_seq_len, tgt_seq_len) - Optional target mask.\n",
    "\n",
    "        Returns:\n",
    "            (batch_size, tgt_seq_len, vocab_size) - Token probabilities.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class TransformerTrainer:\n",
    "    def __init__(self, model: Transformer, learning_rate: float, weight_decay: float):\n",
    "        \"\"\"\n",
    "        Initializes optimizer and loss function for training.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def train_step(self, src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Runs a single training step.\n",
    "\n",
    "        Args:\n",
    "            src: (batch_size, src_seq_len) - Source sequence.\n",
    "            tgt: (batch_size, tgt_seq_len) - Target sequence.\n",
    "\n",
    "        Returns:\n",
    "            Loss value.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, src: torch.Tensor, tgt: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Evaluates the model on a validation set.\n",
    "\n",
    "        Args:\n",
    "            src: (batch_size, src_seq_len) - Source sequence.\n",
    "            tgt: (batch_size, tgt_seq_len) - Target sequence.\n",
    "\n",
    "        Returns:\n",
    "            BLEU score or another evaluation metric.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention-is-all-you-need",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
