{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up an Anaconda environment for implementing the Transformer model in PyTorch, follow these steps:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Create a New Conda Environment**\n",
    "Open a terminal and run:\n",
    "```bash\n",
    "conda create --name attention-is-all-you-need python=3.12\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Activate the Environment**\n",
    "```bash\n",
    "conda activate attention-is-all-you-need\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Install PyTorch**\n",
    "For GPU (CUDA):\n",
    "```bash\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "```\n",
    "For CPU (if you donâ€™t have a compatible GPU):\n",
    "```bash\n",
    "conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
    "```\n",
    "Check if PyTorch is installed correctly:\n",
    "```python\n",
    "python -c \"import torch; print(torch.__version__)\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Install Essential Libraries**\n",
    "```bash\n",
    "pip install numpy pandas matplotlib tqdm\n",
    "```\n",
    "- `numpy`: Tensor operations\n",
    "- `pandas`: Data handling (optional, useful for datasets)\n",
    "- `matplotlib`: Visualization\n",
    "- `tqdm`: Progress bars for training\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Install NLP Libraries (If Needed)**\n",
    "```bash\n",
    "pip install transformers datasets tokenizers sentencepiece\n",
    "```\n",
    "- `transformers`: Pretrained models from Hugging Face (optional)\n",
    "- `datasets`: NLP datasets from Hugging Face\n",
    "- `tokenizers`: Efficient tokenization\n",
    "- `sentencepiece`: Subword tokenization (used in original Transformer)\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Install Jupyter Notebook (Optional)**\n",
    "If you want to develop in Jupyter:\n",
    "```bash\n",
    "conda install jupyter\n",
    "```\n",
    "Then start Jupyter:\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Verify Everything**\n",
    "Run the following to ensure your environment is properly set up:\n",
    "```python\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Save the Environment (Optional)**\n",
    "To export your environment for reproducibility:\n",
    "```bash\n",
    "conda env export > environment.yml\n",
    "```\n",
    "To recreate it later:\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int):\n",
    "        \"\"\"\n",
    "        Initializes the embedding layer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Number of unique tokens in the vocabulary.\n",
    "            d_model (int): Dimension of the embedding vectors.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Define the embedding layer that maps token indices to dense vectors.\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)  \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for token embedding.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor of shape (batch_size, seq_len) containing token indices.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of shape (batch_size, seq_len, d_model) containing embedded representations.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the lookup operation using the embedding layer.\n",
    "        embedded = self.embedding(x)  \n",
    "\n",
    "        return embedded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def run_tests():\n",
    "    # Test Parameters\n",
    "    vocab_size = 100\n",
    "    d_model = 16\n",
    "    batch_size = 4\n",
    "    seq_len = 10\n",
    "\n",
    "    # Create a sample input tensor\n",
    "    test_input = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "    # Initialize TokenEmbedding\n",
    "    embedding_layer = TokenEmbedding(vocab_size, d_model)\n",
    "\n",
    "    # Test 1: Check Output Shape\n",
    "    output = embedding_layer(test_input)\n",
    "    assert output.shape == (batch_size, seq_len, d_model), f\"Unexpected shape: {output.shape}\"\n",
    "    \n",
    "    # Test 2: Ensure Output is a Tensor of Correct Type\n",
    "    assert isinstance(output, torch.Tensor), \"Output is not a tensor\"\n",
    "    assert output.dtype == torch.float32, f\"Unexpected dtype: {output.dtype}\"\n",
    "    \n",
    "    # Test 3: Check if the Same Token Index Maps to the Same Embedding\n",
    "    index = torch.tensor([[5]])\n",
    "    embedding_1 = embedding_layer(index)\n",
    "    embedding_2 = embedding_layer(index)\n",
    "    assert torch.allclose(embedding_1, embedding_2), \"Embeddings should be identical for the same index\"\n",
    "    \n",
    "    # Test 4: Check if Different Indices Give Different Embeddings\n",
    "    index1 = torch.tensor([[5]])\n",
    "    index2 = torch.tensor([[8]])\n",
    "    embedding_1 = embedding_layer(index1)\n",
    "    embedding_2 = embedding_layer(index2)\n",
    "    assert not torch.allclose(embedding_1, embedding_2), \"Different indices should have different embeddings\"\n",
    "    \n",
    "    # Test 5: Check if Gradients are Computed\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    assert embedding_layer.embedding.weight.grad is not None, \"Gradients should not be None\"\n",
    "    assert embedding_layer.embedding.weight.grad.shape == (vocab_size, d_model), \"Gradient shape mismatch\"\n",
    "    \n",
    "    print(\"âœ… All tests passed successfully!\")\n",
    "\n",
    "# Run all tests\n",
    "run_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5409, -0.4400,  0.0846], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = TokenEmbedding(vocab_size=10, d_model=3)\n",
    "embedding_layer(torch.tensor(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # Create a (max_len, d_model) tensor to hold the positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)            # shape: (max_len, d_model)\n",
    "        \n",
    "        # position: shape (max_len, 1)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # div_term: shape (d_model/2,)  -> weâ€™ll use it for the even/odd splits\n",
    "        # This follows exp(- log(10000) * (2i/d_model)) = 10000^(-2i/d_model).\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        # Apply sine to even indices (0, 2, 4, ...)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # Apply cosine to odd indices (1, 3, 5, ...)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register 'pe' as a buffer so it's not trained\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: shape (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # Grab up to seq_len positions from pe and add to x\n",
    "        # shape of pe_slice becomes (1, seq_len, d_model)\n",
    "        pe_slice = self.pe[:seq_len, :].unsqueeze(0).to(x.device)\n",
    "\n",
    "        return x + pe_slice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All positional encoding tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def run_positional_encoding_tests():\n",
    "    d_model = 16\n",
    "    seq_len = 10\n",
    "    batch_size = 4\n",
    "\n",
    "    test_input = torch.zeros((batch_size, seq_len, d_model))  # Placeholder embeddings\n",
    "    pos_encoding = PositionalEncoding(d_model=d_model)\n",
    "\n",
    "    # Test 1: Check Output Shape\n",
    "    output = pos_encoding(test_input)\n",
    "    assert output.shape == (batch_size, seq_len, d_model), f\"Unexpected shape: {output.shape}\"\n",
    "    \n",
    "    # Test 2: Ensure Output is a Tensor of Correct Type\n",
    "    assert isinstance(output, torch.Tensor), \"Output is not a tensor\"\n",
    "    assert output.dtype == torch.float32, f\"Unexpected dtype: {output.dtype}\"\n",
    "    \n",
    "    # Test 3: Check if Positional Encoding is Being Added\n",
    "    assert not torch.allclose(test_input, output), \"Positional encoding is not being added!\"\n",
    "    \n",
    "    # Test 4: Check Device Compatibility\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    test_input = test_input.to(device)\n",
    "    pos_encoding = pos_encoding.to(device)\n",
    "    output = pos_encoding(test_input)\n",
    "    assert output.device == test_input.device, f\"Device mismatch: {output.device} vs {test_input.device}\"\n",
    "    \n",
    "    # Test 5: Check if Encodings are Deterministic\n",
    "    output1 = pos_encoding(test_input)\n",
    "    output2 = pos_encoding(test_input)\n",
    "    assert torch.allclose(output1, output2), \"Positional encoding should be deterministic!\"\n",
    "    \n",
    "    print(\"âœ… All positional encoding tests passed successfully!\")\n",
    "\n",
    "# Run all tests\n",
    "run_positional_encoding_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+01],\n",
      "        [2.0869e+01],\n",
      "        [5.5094e+01],\n",
      "        [1.9833e+02],\n",
      "        [1.0751e+03],\n",
      "        [1.0000e+04],\n",
      "        [1.8968e+05],\n",
      "        [9.2131e+06],\n",
      "        [1.5473e+09],\n",
      "        [1.3358e+12],\n",
      "        [1.0000e+16],\n",
      "        [1.2946e+21],\n",
      "        [7.2048e+27]])\n"
     ]
    }
   ],
   "source": [
    "def scratchboard(max_len, d_model):\n",
    "    pe = torch.zeros(size=(max_len, d_model))\n",
    "    positions = torch.arange(max_len).unsqueeze(1)\n",
    "    div_term = 10**4**(2*positions/d_model)\n",
    "    print(div_term)\n",
    "\n",
    "scratchboard(13, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k: int):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k   # for scaling\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,   # (batch_size, num_heads, seq_len, d_k)\n",
    "        key: torch.Tensor,     # (batch_size, num_heads, seq_len, d_k)\n",
    "        value: torch.Tensor,   # (batch_size, num_heads, seq_len, d_v)\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # (1) QK^T\n",
    "        # key.transpose(-2, -1) is shape (batch_size, num_heads, d_k, seq_len)\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        \n",
    "        # (2) Scale by sqrt(d_k)\n",
    "        attention_scores = attention_scores / math.sqrt(self.d_k)\n",
    "\n",
    "        # (3) If mask is provided, set masked positions to -inf\n",
    "        if mask is not None:\n",
    "            # Typically a 1/0 mask is used; we want to fill 0â€™s with -inf\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # (4) Apply softmax over the last dimension (seq_len of the key)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # (5) Multiply by V\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledDotProductAttention unit test passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def test_scaled_dot_product_attention():\n",
    "    # Make some deterministic random data.\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    batch_size = 2\n",
    "    num_heads = 3\n",
    "    seq_len_q = 4  # length of the query\n",
    "    seq_len_k = 5  # length of the key\n",
    "    d_k = 6        # dimension per head for query/key\n",
    "    d_v = 6        # dimension per head for value\n",
    "\n",
    "    # Create a random ScaledDotProductAttention instance\n",
    "    attention_module = ScaledDotProductAttention(d_k)\n",
    "\n",
    "    # Create random query, key, value\n",
    "    query = torch.randn(batch_size, num_heads, seq_len_q, d_k)\n",
    "    key   = torch.randn(batch_size, num_heads, seq_len_k, d_k)\n",
    "    value = torch.randn(batch_size, num_heads, seq_len_k, d_v)\n",
    "\n",
    "    # (1) Test forward pass without mask\n",
    "    output, attn_weights = attention_module(query, key, value, mask=None)\n",
    "    \n",
    "    #  -- Check output shape = (batch_size, num_heads, seq_len_q, d_v)\n",
    "    assert output.shape == (batch_size, num_heads, seq_len_q, d_v), \\\n",
    "        f\"Output shape mismatch. Got {output.shape}\"\n",
    "    \n",
    "    #  -- Check attention weight shape = (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    assert attn_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k), \\\n",
    "        f\"Attention weights shape mismatch. Got {attn_weights.shape}\"\n",
    "    \n",
    "    #  -- Check attention weights sum to ~1 across last dimension\n",
    "    attn_sum = attn_weights.sum(dim=-1)\n",
    "    assert torch.allclose(attn_sum, torch.ones_like(attn_sum), atol=1e-5), \\\n",
    "        \"Attention weights do not sum to 1 along the last dimension.\"\n",
    "    \n",
    "    # (2) Test forward pass with a mask (e.g., masking out the last two positions)\n",
    "    #     We'll create a mask of shape (batch_size, 1, seq_len_q, seq_len_k).\n",
    "    #     Suppose we only want the first 3 positions of the key unmasked:\n",
    "    mask = torch.ones(batch_size, 1, seq_len_q, seq_len_k)\n",
    "    mask[:, :, :, -2:] = 0  # mask out the last 2 positions\n",
    "    output_masked, attn_weights_masked = attention_module(query, key, value, mask=mask)\n",
    "\n",
    "    #  -- The masked positions in the softmax should drop to near 0\n",
    "    #     Weâ€™ll check the last two positions of each attention row in attn_weights_masked\n",
    "    #     are effectively 0 (within a floating tolerance).\n",
    "    masked_positions = attn_weights_masked[..., -2:]  # shape (batch_size, num_heads, seq_len_q, 2)\n",
    "    assert torch.allclose(masked_positions, torch.zeros_like(masked_positions), atol=1e-5), \\\n",
    "        \"Masking does not appear to zero out the last two positions.\"\n",
    "\n",
    "    print(\"ScaledDotProductAttention unit test passed!\")\n",
    "\n",
    "\n",
    "# Example usage in a Jupyter cell:\n",
    "test_scaled_dot_product_attention()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        \"\"\"\n",
    "        Initializes multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimension of the model (input and output size).\n",
    "            num_heads (int): Number of attention heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Ensure d_model is divisible by num_heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "\n",
    "        # TODO: Define linear transformations for query, key, and value\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # Replace with nn.Linear\n",
    "        self.W_k = nn.Linear(d_model, d_model)  # Replace with nn.Linear\n",
    "        self.W_v = nn.Linear(d_model, d_model)  # Replace with nn.Linear\n",
    "\n",
    "        # TODO: Define output projection layer\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # Replace with nn.Linear\n",
    "\n",
    "        # TODO: Define the scaled dot-product attention module\n",
    "        self.attention = ScaledDotProductAttention(self.d_k)  # Replace with ScaledDotProductAttention(self.d_k)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): Shape (batch_size, seq_len, d_model)\n",
    "            key (torch.Tensor): Shape (batch_size, seq_len, d_model)\n",
    "            value (torch.Tensor): Shape (batch_size, seq_len, d_model)\n",
    "            mask (Optional[torch.Tensor]): Shape (batch_size, 1, seq_len, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Shape (batch_size, seq_len, d_model) - Multi-head attention output.\n",
    "        \"\"\"\n",
    "        # TODO: Apply linear transformations to query, key, and value\n",
    "        Q = self.W_q(query)  # Replace with correct transformation\n",
    "        K = self.W_k(key)  # Replace with correct transformation\n",
    "        V = self.W_v(value)  # Replace with correct transformation\n",
    "\n",
    "        # TODO: Reshape Q, K, V for multi-head attention\n",
    "        # Hint: Use `.view()` and `.transpose()` to shape into (batch_size, num_heads, seq_len, d_k)\n",
    "        batch_size, seq_len, _ = query.shape\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # TODO: Apply scaled dot-product attention\n",
    "        output, attention_weights = self.attention(Q, K, V, mask)  # Replace with correct computation\n",
    "\n",
    "        # TODO: Concatenate the heads back and apply final linear transformation\n",
    "        # Current shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        # We first swap num_heads and seq_len\n",
    "        output = output.transpose(1, 2)  # (batch_size, seq_len, num_heads, d_k)\n",
    "        output = output.contiguous().view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        output = self.W_o(output)  # Replace with correct transformation\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Output shape test passed!\n",
      "âœ… Tensor type test passed!\n",
      "âœ… Deterministic output test passed!\n",
      "âœ… Masking test passed!\n",
      "ðŸŽ‰ All MultiHeadAttention tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_multi_head_attention():\n",
    "    \"\"\"\n",
    "    Tests the MultiHeadAttention module for correctness.\n",
    "    \"\"\"\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "    d_model = 16\n",
    "    num_heads = 4\n",
    "\n",
    "    # Initialize test input tensors\n",
    "    query = torch.randn(batch_size, seq_len, d_model)\n",
    "    key = torch.randn(batch_size, seq_len, d_model)\n",
    "    value = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # Initialize multi-head attention module\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    # Run forward pass without a mask\n",
    "    output = mha(query, key, value, mask=None)\n",
    "\n",
    "    # Test 1: Check output shape\n",
    "    assert output.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Unexpected output shape: {output.shape}\"\n",
    "    print(\"âœ… Output shape test passed!\")\n",
    "\n",
    "    # Test 2: Ensure output is a tensor\n",
    "    assert isinstance(output, torch.Tensor), \"Output is not a tensor\"\n",
    "    print(\"âœ… Tensor type test passed!\")\n",
    "\n",
    "    # Test 3: Check deterministic output for same input\n",
    "    output_2 = mha(query, key, value, mask=None)\n",
    "    assert torch.allclose(output, output_2), \"Output should be deterministic!\"\n",
    "    print(\"âœ… Deterministic output test passed!\")\n",
    "\n",
    "    # Test 4: Apply a mask and check if masking works\n",
    "    mask = torch.zeros(batch_size, 1, seq_len, seq_len)\n",
    "    mask[:, :, :, -1] = float('-inf')  # Mask the last token\n",
    "\n",
    "    output_masked = mha(query, key, value, mask=mask)\n",
    "\n",
    "    # Ensure output is still the correct shape\n",
    "    assert output_masked.shape == (batch_size, seq_len, d_model), \\\n",
    "        \"Masked output shape mismatch\"\n",
    "    print(\"âœ… Masking test passed!\")\n",
    "\n",
    "    print(\"ðŸŽ‰ All MultiHeadAttention tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_multi_head_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model) - Transformed representations.\n",
    "        \"\"\"\n",
    "        output = self.fc1(x)\n",
    "        output = self.relu(output)\n",
    "        output = self.fc2(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Output shape test passed!\n",
      "âœ… Tensor type test passed!\n",
      "âœ… ReLU activation test passed!\n",
      "âœ… Deterministic output test passed!\n",
      "âœ… Gradient computation test passed!\n",
      "ðŸŽ‰ All PositionwiseFeedForward tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_positionwise_feedforward():\n",
    "    \"\"\"\n",
    "    Tests the PositionwiseFeedForward module.\n",
    "    \"\"\"\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "    d_model = 16\n",
    "    d_ff = 32  # Expanded dimension\n",
    "\n",
    "    # Initialize test input tensor (random)\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # Initialize the feed-forward module\n",
    "    ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "\n",
    "    # Run forward pass\n",
    "    output = ffn(x)\n",
    "\n",
    "    # Test 1: Check output shape\n",
    "    assert output.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Unexpected output shape: {output.shape}\"\n",
    "    print(\"âœ… Output shape test passed!\")\n",
    "\n",
    "    # Test 2: Ensure output is a tensor\n",
    "    assert isinstance(output, torch.Tensor), \"Output is not a tensor\"\n",
    "    print(\"âœ… Tensor type test passed!\")\n",
    "\n",
    "    # Test 3: Ensure ReLU activation is applied\n",
    "    hidden_layer_output = ffn.fc1(x)  # Get pre-ReLU values\n",
    "    assert torch.all((hidden_layer_output > 0) == (ffn.relu(hidden_layer_output) > 0)), \\\n",
    "        \"ReLU activation is not applied correctly\"\n",
    "    print(\"âœ… ReLU activation test passed!\")\n",
    "\n",
    "    # Test 4: Check deterministic output for same input\n",
    "    output_2 = ffn(x)\n",
    "    assert torch.allclose(output, output_2), \"Output should be deterministic!\"\n",
    "    print(\"âœ… Deterministic output test passed!\")\n",
    "\n",
    "    # Test 5: Check gradients (ensuring backpropagation works)\n",
    "    output.sum().backward()  # Compute gradients\n",
    "    assert ffn.fc1.weight.grad is not None, \"Gradients are not computed for fc1!\"\n",
    "    assert ffn.fc2.weight.grad is not None, \"Gradients are not computed for fc2!\"\n",
    "    print(\"âœ… Gradient computation test passed!\")\n",
    "\n",
    "    print(\"ðŸŽ‰ All PositionwiseFeedForward tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_positionwise_feedforward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initializes a single Transformer Encoder Layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The embedding dimension (must be divisible by num_heads).\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Hidden layer size of the feed-forward network.\n",
    "            dropout (float): Dropout rate (default 0.1).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Define multi-head self-attention layer\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)  # Replace with MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # TODO: Define feed-forward network (FFN)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff)  # Replace with a two-layer FFN\n",
    "\n",
    "        # TODO: Define Layer Normalization layers\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model) \n",
    "\n",
    "        # TODO: Define Dropout layers\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for Transformer Encoder Layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "            mask (Optional[torch.Tensor]): Mask for attention (default None).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_len, d_model).\n",
    "        \"\"\"\n",
    "        # TODO: Apply multi-head self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask=mask) \n",
    "\n",
    "        # TODO: Apply residual connection and layer normalization\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "\n",
    "        # TODO: Apply feed-forward network\n",
    "        ffn_output = self.ffn(x) \n",
    "\n",
    "        # TODO: Apply second residual connection and layer normalization\n",
    "        x = self.norm2(x + self.dropout2(ffn_output))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Instantiation test passed!\n",
      "âœ… Forward pass (no mask) shape test passed!\n",
      "âœ… Forward pass (with mask) shape test passed!\n",
      "âœ… ReLU activation test passed!\n",
      "âœ… Backward pass (gradient) test passed!\n",
      "All TransformerEncoderLayer tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytest\n",
    "\n",
    "def test_transformer_encoder_layer():\n",
    "    \"\"\"\n",
    "    Basic tests for TransformerEncoderLayer:\n",
    "    1) Instantiation\n",
    "    2) Forward pass shape (with and without masks)\n",
    "    3) Presence of ReLU (if expected in FFN)\n",
    "    4) Gradient backprop\n",
    "    \"\"\"\n",
    "    # -------------------\n",
    "    # Hyperparams\n",
    "    d_model = 32\n",
    "    num_heads = 4\n",
    "    d_ff = 64\n",
    "    dropout = 0.1\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "\n",
    "    # -------------------\n",
    "    # 1) Instantiate layer\n",
    "    try:\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Instantiation failed with error: {e}\")\n",
    "\n",
    "    print(\"âœ… Instantiation test passed!\")\n",
    "\n",
    "    # -------------------\n",
    "    # Prepare dummy input\n",
    "    x = torch.randn(batch_size, seq_len, d_model)  # (B, S, d_model)\n",
    "\n",
    "    # -------------------\n",
    "    # 2) Forward pass (no mask)\n",
    "    try:\n",
    "        output_no_mask = encoder_layer(x)  # no mask\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed without mask: {e}\")\n",
    "\n",
    "    # Check shape\n",
    "    assert output_no_mask.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape {output_no_mask.shape} != {(batch_size, seq_len, d_model)}\"\n",
    "    print(\"âœ… Forward pass (no mask) shape test passed!\")\n",
    "\n",
    "    # -------------------\n",
    "    # 3) Forward pass (with mask)\n",
    "    # Example: a binary mask that \"allows\" everything (all 1s). \n",
    "    # Must match the shape your MultiHeadAttention expects, typically (B, 1, S, S).\n",
    "    mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
    "\n",
    "    try:\n",
    "        output_with_mask = encoder_layer(x, mask=mask)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed with mask: {e}\")\n",
    "\n",
    "    # Check shape\n",
    "    assert output_with_mask.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape {output_with_mask.shape} != {(batch_size, seq_len, d_model)}\"\n",
    "    print(\"âœ… Forward pass (with mask) shape test passed!\")\n",
    "\n",
    "    # -------------------\n",
    "    # 4) Check for ReLU activation in feed-forward (optional)\n",
    "    #    If you're using a different activation, change accordingly.\n",
    "    found_relu = False\n",
    "    for submodule in encoder_layer.modules():\n",
    "        if isinstance(submodule, nn.ReLU):\n",
    "            found_relu = True\n",
    "            break\n",
    "    assert found_relu, (\n",
    "        \"No ReLU found in TransformerEncoderLayer's FFN (if you expected one). \"\n",
    "        \"If using a different activation, adjust this test.\"\n",
    "    )\n",
    "    print(\"âœ… ReLU activation test passed!\")\n",
    "\n",
    "    # -------------------\n",
    "    # 5) Gradient check\n",
    "    #    Ensure we can do a backward pass without errors\n",
    "    output_with_mask.sum().backward()\n",
    "    print(\"âœ… Backward pass (gradient) test passed!\")\n",
    "\n",
    "    print(\"All TransformerEncoderLayer tests passed successfully!\")\n",
    "\n",
    "test_transformer_encoder_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module): \n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Initializes a single Transformer Decoder Layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The embedding dimension (must be divisible by num_heads).\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Hidden layer size of the feed-forward network.\n",
    "            dropout (float): Dropout rate (default 0.1).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Define masked multi-head self-attention layer\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # TODO: Define multi-head attention layer for encoder-decoder attention\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)  # Replace with MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # TODO: Define feed-forward network (FFN)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "\n",
    "        # TODO: Define Layer Normalization layers\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model) \n",
    "        self.norm3 = nn.LayerNorm(d_model) \n",
    "\n",
    "        # TODO: Define Dropout layers\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, memory: torch.Tensor, \n",
    "                src_mask: Optional[torch.Tensor] = None, \n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for Transformer Decoder Layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model) (decoder input).\n",
    "            memory (torch.Tensor): Encoder outputs of shape (batch_size, seq_len_enc, d_model).\n",
    "            tgt_mask (Optional[torch.Tensor]): Mask for target self-attention (default None).\n",
    "            src_mask (Optional[torch.Tensor]): Mask for encoder-decoder attention (default None).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_len, d_model).\n",
    "        \"\"\"\n",
    "        # TODO: Apply masked multi-head self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask=tgt_mask) \n",
    "\n",
    "        # TODO: Apply residual connection and layer normalization\n",
    "        x = self.norm1(attn_output + self.dropout1(x))\n",
    "\n",
    "        # TODO: Apply encoder-decoder multi-head attention\n",
    "        attn_output_2, _ = self.enc_dec_attn(memory, memory, x, mask=src_mask)\n",
    "\n",
    "        # TODO: Apply residual connection and layer normalization\n",
    "        x = self.norm2(attn_output_2 + self.dropout2(x))\n",
    "\n",
    "        # TODO: Apply feed-forward network\n",
    "        ffn_output = self.ffn(x)\n",
    "\n",
    "        # TODO: Apply final residual connection and layer normalization\n",
    "        x = self.norm3(ffn_output + self.dropout3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Instantiation test passed!\n",
      "âœ… Forward pass (no masks) shape test passed!\n",
      "âœ… Forward pass (with masks) shape test passed!\n",
      "âœ… ReLU activation test passed!\n",
      "âœ… Backward pass (gradient) test passed!\n",
      "All TransformerDecoderLayer tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytest\n",
    "\n",
    "def test_transformer_decoder_layer():\n",
    "    \"\"\"\n",
    "    Basic tests for TransformerDecoderLayer to check:\n",
    "    1) Instantiation without errors\n",
    "    2) Forward pass shape consistency\n",
    "    3) Handling of optional masks\n",
    "    4) Presence of ReLU activation (if expected)\n",
    "    5) Gradient backprop flow\n",
    "    \"\"\"\n",
    "    d_model = 32\n",
    "    num_heads = 4\n",
    "    d_ff = 64\n",
    "    dropout = 0.1\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "\n",
    "    # 1) Instantiate the layer\n",
    "    try:\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Instantiation failed with error: {e}\")\n",
    "\n",
    "    print(\"âœ… Instantiation test passed!\")\n",
    "\n",
    "    # 2) Create dummy inputs\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    memory = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # 3) Forward pass shape test without masks\n",
    "    try:\n",
    "        output = decoder_layer(x, memory)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed without masks: {e}\")\n",
    "\n",
    "    # Check shape\n",
    "    assert output.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape {output.shape} != {(batch_size, seq_len, d_model)}\"\n",
    "    print(\"âœ… Forward pass (no masks) shape test passed!\")\n",
    "\n",
    "    # 4) Forward pass with random masks\n",
    "    src_mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
    "    tgt_mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
    "    try:\n",
    "        output_masked = decoder_layer(x, memory, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed with masks: {e}\")\n",
    "\n",
    "    # Check shape again\n",
    "    assert output_masked.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape with masks {output_masked.shape} != {(batch_size, seq_len, d_model)}\"\n",
    "    print(\"âœ… Forward pass (with masks) shape test passed!\")\n",
    "\n",
    "    # 5) (Optional) Check for ReLU activation in the layer\n",
    "    #    Depending on your exact implementation, you may not have a direct ReLU submodule.\n",
    "    found_relu = False\n",
    "    for mod in decoder_layer.modules():\n",
    "        if isinstance(mod, nn.ReLU):\n",
    "            found_relu = True\n",
    "            break\n",
    "    assert found_relu, \"No ReLU found in the TransformerDecoderLayer (if you expected one)!\"\n",
    "    print(\"âœ… ReLU activation test passed!\")\n",
    "\n",
    "    # 6) Quick gradient test\n",
    "    #    Make sure we can do a backward pass without errors\n",
    "    output_sum = output_masked.sum()\n",
    "    try:\n",
    "        output_sum.backward()\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Backward pass failed: {e}\")\n",
    "\n",
    "    print(\"âœ… Backward pass (gradient) test passed!\")\n",
    "\n",
    "    print(\"All TransformerDecoderLayer tests passed successfully!\")\n",
    "\n",
    "test_transformer_decoder_layer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initializes a Transformer Encoder consisting of multiple encoder layers.\n",
    "\n",
    "        Args:\n",
    "            num_layers (int): Number of TransformerEncoderLayer layers.\n",
    "            d_model (int): Dimension of embeddings and model size.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Hidden layer size in feed-forward network.\n",
    "            dropout (float): Dropout rate (default 0.1).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Define a stack of TransformerEncoderLayers\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])  \n",
    "\n",
    "        # TODO: Define final layer normalization\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the Transformer Encoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "            mask (Optional[torch.Tensor]): Optional mask for attention.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Encoded representation of shape (batch_size, seq_len, d_model).\n",
    "        \"\"\"\n",
    "        # TODO: Pass input through each TransformerEncoderLayer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # TODO: Apply final normalization\n",
    "        x = self.norm(x)  \n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Instantiation test passed!\n",
      "âœ… Forward pass (no mask) shape test passed!\n",
      "âœ… Forward pass (with mask) shape test passed!\n",
      "âœ… ReLU activation test passed!\n",
      "âœ… Backward pass (gradient) test passed!\n",
      "All TransformerEncoder tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytest\n",
    "\n",
    "def test_transformer_encoder():\n",
    "    \"\"\"\n",
    "    Basic tests for TransformerEncoder to check:\n",
    "    1) Instantiation without errors\n",
    "    2) Forward pass shape consistency (with and without masks)\n",
    "    3) Presence of ReLU activation (if expected)\n",
    "    4) Gradient backprop flow\n",
    "    \"\"\"\n",
    "    # Hyperparameters and dummy inputs\n",
    "    num_layers = 2\n",
    "    d_model = 32\n",
    "    num_heads = 4\n",
    "    d_ff = 64\n",
    "    dropout = 0.1\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "\n",
    "    # 1) Instantiate the encoder\n",
    "    try:\n",
    "        encoder = TransformerEncoder(num_layers, d_model, num_heads, d_ff, dropout)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Instantiation failed with error: {e}\")\n",
    "\n",
    "    print(\"âœ… Instantiation test passed!\")\n",
    "\n",
    "    # Create dummy inputs\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # 2) Forward pass shape test without mask\n",
    "    try:\n",
    "        output_no_mask = encoder(x)  # no mask\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed without mask: {e}\")\n",
    "\n",
    "    # Check shape\n",
    "    assert output_no_mask.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape {output_no_mask.shape} != {(batch_size, seq_len, d_model)}\"\n",
    "    print(\"âœ… Forward pass (no mask) shape test passed!\")\n",
    "\n",
    "    # 3) Forward pass with a dummy mask\n",
    "    mask = torch.ones(batch_size, 1, seq_len, seq_len)  # e.g., all ones as a placeholder\n",
    "    try:\n",
    "        output_with_mask = encoder(x, mask)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed with mask: {e}\")\n",
    "\n",
    "    assert output_with_mask.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape with mask {output_with_mask.shape} != {(batch_size, seq_len, d_model)}\"\n",
    "    print(\"âœ… Forward pass (with mask) shape test passed!\")\n",
    "\n",
    "    # 4) (Optional) Check for ReLU activation in the encoder\n",
    "    #    Adjust this if your encoder uses a different activation function.\n",
    "    found_relu = False\n",
    "    for mod in encoder.modules():\n",
    "        if isinstance(mod, nn.ReLU):\n",
    "            found_relu = True\n",
    "            break\n",
    "    assert found_relu, \"No ReLU found in the TransformerEncoder (if you expected one)!\"\n",
    "    print(\"âœ… ReLU activation test passed!\")\n",
    "\n",
    "    # 5) Quick gradient test\n",
    "    #    Make sure backward pass works\n",
    "    output_with_mask.sum().backward()  # should not error out\n",
    "    print(\"âœ… Backward pass (gradient) test passed!\")\n",
    "\n",
    "    print(\"All TransformerEncoder tests passed successfully!\")\n",
    "\n",
    "test_transformer_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initializes a Transformer Decoder consisting of multiple decoder layers.\n",
    "\n",
    "        Args:\n",
    "            num_layers (int): Number of TransformerDecoderLayer layers.\n",
    "            d_model (int): Dimension of embeddings and model size.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Hidden layer size in feed-forward network.\n",
    "            dropout (float): Dropout rate (default 0.1).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Define a stack of TransformerDecoderLayers\n",
    "        self.layers = nn.ModuleList([TransformerDecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])  # Replace with nn.ModuleList([...])\n",
    "\n",
    "        # TODO: Define final layer normalization\n",
    "        self.norm = nn.LayerNorm(d_model)  # Replace with nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, memory: torch.Tensor, \n",
    "                tgt_mask: Optional[torch.Tensor] = None, \n",
    "                src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the Transformer Decoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Decoder input tensor (batch_size, tgt_seq_len, d_model).\n",
    "            memory (torch.Tensor): Encoder outputs (batch_size, src_seq_len, d_model).\n",
    "            tgt_mask (Optional[torch.Tensor]): Mask for target self-attention.\n",
    "            src_mask (Optional[torch.Tensor]): Mask for encoder-decoder attention.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Decoded representation of shape (batch_size, tgt_seq_len, d_model).\n",
    "        \"\"\"\n",
    "        # TODO: Pass input through each TransformerDecoderLayer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, tgt_mask, src_mask)  # Replace with layer(x, memory, tgt_mask, src_mask)\n",
    "\n",
    "        # TODO: Apply final normalization\n",
    "        x = self.norm(x)  # Replace with self.norm(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Instantiation test passed!\n",
      "âœ… Forward pass (no masks) shape test passed!\n",
      "âœ… Forward pass (with masks) shape test passed!\n",
      "âœ… ReLU activation test passed!\n",
      "âœ… Backward pass (gradient) test passed!\n",
      "All TransformerDecoder tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytest\n",
    "\n",
    "def test_transformer_decoder():\n",
    "    \"\"\"\n",
    "    Basic tests for TransformerDecoder to check:\n",
    "    1) Instantiation\n",
    "    2) Forward pass shape consistency (with and without masks)\n",
    "    3) Presence of ReLU activation (if expected)\n",
    "    4) Gradient backprop\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyperparameters & dummy input sizes\n",
    "    num_layers = 2\n",
    "    d_model = 32\n",
    "    num_heads = 4\n",
    "    d_ff = 64\n",
    "    dropout = 0.1\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "\n",
    "    # 1) Instantiate the decoder\n",
    "    try:\n",
    "        decoder = TransformerDecoder(num_layers, d_model, num_heads, d_ff, dropout)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Instantiation failed with error: {e}\")\n",
    "\n",
    "    print(\"âœ… Instantiation test passed!\")\n",
    "\n",
    "    # Create dummy inputs\n",
    "    x = torch.randn(batch_size, seq_len, d_model)      # Target sequence\n",
    "    memory = torch.randn(batch_size, seq_len, d_model) # Encoder output\n",
    "\n",
    "    # 2) Forward pass without masks\n",
    "    try:\n",
    "        output_no_mask = decoder(x, memory)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed without masks: {e}\")\n",
    "\n",
    "    # Check shape\n",
    "    assert output_no_mask.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape {output_no_mask.shape} != {(batch_size, seq_len, d_model)}\"\n",
    "    print(\"âœ… Forward pass (no masks) shape test passed!\")\n",
    "\n",
    "    # 3) Forward pass with masks\n",
    "    src_mask = torch.ones(batch_size, 1, seq_len, seq_len)  # dummy encoder mask\n",
    "    tgt_mask = torch.ones(batch_size, 1, seq_len, seq_len)  # dummy decoder mask\n",
    "    try:\n",
    "        output_with_masks = decoder(x, memory, src_mask, tgt_mask)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed with masks: {e}\")\n",
    "\n",
    "    # Check shape again\n",
    "    assert output_with_masks.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape with masks {output_with_masks.shape} != {(batch_size, seq_len, d_model)}\"\n",
    "    print(\"âœ… Forward pass (with masks) shape test passed!\")\n",
    "\n",
    "    # 4) (Optional) Check for ReLU activation in the decoder\n",
    "    #    Adjust this if your decoder uses a different activation\n",
    "    found_relu = False\n",
    "    for mod in decoder.modules():\n",
    "        if isinstance(mod, nn.ReLU):\n",
    "            found_relu = True\n",
    "            break\n",
    "    assert found_relu, \"No ReLU found in the TransformerDecoder (if you expected one)!\"\n",
    "    print(\"âœ… ReLU activation test passed!\")\n",
    "\n",
    "    # 5) Quick gradient test\n",
    "    #    Make sure backward pass works\n",
    "    output_with_masks.sum().backward()  # Should not raise an error\n",
    "    print(\"âœ… Backward pass (gradient) test passed!\")\n",
    "\n",
    "    print(\"All TransformerDecoder tests passed successfully!\")\n",
    "    \n",
    "test_transformer_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, num_layers: int, num_heads: int, \n",
    "                 d_ff: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Number of unique tokens in the vocabulary.\n",
    "            d_model (int): Embedding dimension.\n",
    "            num_layers (int): Number of encoder and decoder layers.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Hidden layer size in feed-forward network.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Define token embeddings\n",
    "        self.embedding = None  # Replace with nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # TODO: Define positional encoding\n",
    "        self.pos_encoding = None  # Replace with PositionalEncoding(d_model)\n",
    "\n",
    "        # TODO: Define the encoder\n",
    "        self.encoder = None  # Replace with TransformerEncoder(...)\n",
    "\n",
    "        # TODO: Define the decoder\n",
    "        self.decoder = None  # Replace with TransformerDecoder(...)\n",
    "\n",
    "        # TODO: Define the final projection layer\n",
    "        self.fc_out = None  # Replace with nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, \n",
    "                src_mask: Optional[torch.Tensor] = None, \n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): Source token indices of shape (batch_size, src_seq_len).\n",
    "            tgt (torch.Tensor): Target token indices of shape (batch_size, tgt_seq_len).\n",
    "            src_mask (Optional[torch.Tensor]): Source mask of shape (batch_size, 1, src_seq_len, src_seq_len).\n",
    "            tgt_mask (Optional[torch.Tensor]): Target mask of shape (batch_size, 1, tgt_seq_len, tgt_seq_len).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Token probabilities of shape (batch_size, tgt_seq_len, vocab_size).\n",
    "        \"\"\"\n",
    "        # TODO: Apply token embedding and positional encoding\n",
    "        src_emb = None  # Replace with self.embedding(src) + self.pos_encoding(src)\n",
    "        tgt_emb = None  # Replace with self.embedding(tgt) + self.pos_encoding(tgt)\n",
    "\n",
    "        # TODO: Pass through the encoder\n",
    "        memory = None  # Replace with self.encoder(src_emb, src_mask)\n",
    "\n",
    "        # TODO: Pass through the decoder\n",
    "        output = None  # Replace with self.decoder(tgt_emb, memory, tgt_mask, src_mask)\n",
    "\n",
    "        # TODO: Apply final linear layer to project into vocab size\n",
    "        logits = None  # Replace with self.fc_out(output)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Instantiation test passed!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 96\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Backward pass (gradient) test passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll Transformer tests passed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m \u001b[43mtest_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 52\u001b[0m, in \u001b[0;36mtest_transformer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Check shape: (batch_size, tgt_seq_len, vocab_size)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m expected_shape \u001b[38;5;241m=\u001b[39m (batch_size, tgt_seq_len, vocab_size)\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43moutput_no_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m \u001b[38;5;241m==\u001b[39m expected_shape, \\\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_no_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Forward pass (no masks) shape test passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# 3) Forward pass with masks\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Example: random binary masks (1 = keep, 0 = mask). \u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Some implementations expect float masks with -inf for masked positions. \u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Adjust as needed for your attention mechanism.\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytest\n",
    "\n",
    "def test_transformer():\n",
    "    \"\"\"\n",
    "    Basic tests for the Transformer model:\n",
    "    1) Instantiation\n",
    "    2) Forward pass shape (with and without masks)\n",
    "    3) Presence of ReLU (if expected)\n",
    "    4) Gradient backprop\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    vocab_size = 10\n",
    "    d_model = 8\n",
    "    num_layers = 2\n",
    "    num_heads = 2\n",
    "    d_ff = 16\n",
    "    dropout = 0.1\n",
    "\n",
    "    batch_size = 2\n",
    "    src_seq_len = 5\n",
    "    tgt_seq_len = 6\n",
    "\n",
    "    # 1) Instantiate\n",
    "    try:\n",
    "        model = Transformer(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=d_model,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            d_ff=d_ff,\n",
    "            dropout=dropout\n",
    "        )\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Instantiation failed with error: {e}\")\n",
    "\n",
    "    print(\"âœ… Instantiation test passed!\")\n",
    "\n",
    "    # Create dummy source & target inputs (token indices)\n",
    "    src = torch.randint(0, vocab_size, (batch_size, src_seq_len))\n",
    "    tgt = torch.randint(0, vocab_size, (batch_size, tgt_seq_len))\n",
    "\n",
    "    # 2) Forward pass without masks\n",
    "    try:\n",
    "        output_no_mask = model(src, tgt)  # no masks\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed without masks: {e}\")\n",
    "\n",
    "    # Check shape: (batch_size, tgt_seq_len, vocab_size)\n",
    "    expected_shape = (batch_size, tgt_seq_len, vocab_size)\n",
    "    assert output_no_mask.shape == expected_shape, \\\n",
    "        f\"Output shape {output_no_mask.shape} != {expected_shape}\"\n",
    "\n",
    "    print(\"âœ… Forward pass (no masks) shape test passed!\")\n",
    "\n",
    "    # 3) Forward pass with masks\n",
    "    # Example: random binary masks (1 = keep, 0 = mask). \n",
    "    # Some implementations expect float masks with -inf for masked positions. \n",
    "    # Adjust as needed for your attention mechanism.\n",
    "    src_mask = torch.ones(batch_size, 1, src_seq_len, src_seq_len)\n",
    "    tgt_mask = torch.ones(batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
    "\n",
    "    try:\n",
    "        output_with_masks = model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed with masks: {e}\")\n",
    "\n",
    "    assert output_with_masks.shape == expected_shape, \\\n",
    "        f\"Output shape with masks {output_with_masks.shape} != {expected_shape}\"\n",
    "\n",
    "    print(\"âœ… Forward pass (with masks) shape test passed!\")\n",
    "\n",
    "    # 4) Check for ReLU activation in the model (optional)\n",
    "    #    If you use a different activation like GELU, adjust this accordingly.\n",
    "    found_relu = False\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.ReLU):\n",
    "            found_relu = True\n",
    "            break\n",
    "    assert found_relu, \"No ReLU found in Transformer (if you expected it)!\"\n",
    "    print(\"âœ… ReLU activation test passed!\")\n",
    "\n",
    "    # 5) Gradient backprop check\n",
    "    #    Make sure we can run backward without errors\n",
    "    loss = output_with_masks.sum()  # Simple scalar\n",
    "    try:\n",
    "        loss.backward()\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Backward pass failed: {e}\")\n",
    "\n",
    "    print(\"âœ… Backward pass (gradient) test passed!\")\n",
    "\n",
    "    print(\"All Transformer tests passed successfully!\")\n",
    "\n",
    "test_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTrainer:\n",
    "    def __init__(self, model: Transformer, learning_rate: float, weight_decay: float):\n",
    "        \"\"\"\n",
    "        Initializes optimizer and loss function for training.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def train_step(self, src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Runs a single training step.\n",
    "\n",
    "        Args:\n",
    "            src: (batch_size, src_seq_len) - Source sequence.\n",
    "            tgt: (batch_size, tgt_seq_len) - Target sequence.\n",
    "\n",
    "        Returns:\n",
    "            Loss value.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, src: torch.Tensor, tgt: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Evaluates the model on a validation set.\n",
    "\n",
    "        Args:\n",
    "            src: (batch_size, src_seq_len) - Source sequence.\n",
    "            tgt: (batch_size, tgt_seq_len) - Target sequence.\n",
    "\n",
    "        Returns:\n",
    "            BLEU score or another evaluation metric.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention-is-all-you-need",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
