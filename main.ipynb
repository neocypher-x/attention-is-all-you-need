{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up an Anaconda environment for implementing the Transformer model in PyTorch, follow these steps:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Create a New Conda Environment**\n",
    "Open a terminal and run:\n",
    "```bash\n",
    "conda create --name attention-is-all-you-need python=3.12\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Activate the Environment**\n",
    "```bash\n",
    "conda activate attention-is-all-you-need\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Install PyTorch**\n",
    "For GPU (CUDA):\n",
    "```bash\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "```\n",
    "For CPU (if you don’t have a compatible GPU):\n",
    "```bash\n",
    "conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
    "```\n",
    "Check if PyTorch is installed correctly:\n",
    "```python\n",
    "python -c \"import torch; print(torch.__version__)\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Install Essential Libraries**\n",
    "```bash\n",
    "pip install numpy pandas matplotlib tqdm\n",
    "```\n",
    "- `numpy`: Tensor operations\n",
    "- `pandas`: Data handling (optional, useful for datasets)\n",
    "- `matplotlib`: Visualization\n",
    "- `tqdm`: Progress bars for training\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Install NLP Libraries (If Needed)**\n",
    "```bash\n",
    "pip install transformers datasets tokenizers sentencepiece\n",
    "```\n",
    "- `transformers`: Pretrained models from Hugging Face (optional)\n",
    "- `datasets`: NLP datasets from Hugging Face\n",
    "- `tokenizers`: Efficient tokenization\n",
    "- `sentencepiece`: Subword tokenization (used in original Transformer)\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Install Jupyter Notebook (Optional)**\n",
    "If you want to develop in Jupyter:\n",
    "```bash\n",
    "conda install jupyter\n",
    "```\n",
    "Then start Jupyter:\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Verify Everything**\n",
    "Run the following to ensure your environment is properly set up:\n",
    "```python\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Save the Environment (Optional)**\n",
    "To export your environment for reproducibility:\n",
    "```bash\n",
    "conda env export > environment.yml\n",
    "```\n",
    "To recreate it later:\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int):\n",
    "        \"\"\"\n",
    "        Initializes the embedding layer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Number of unique tokens in the vocabulary.\n",
    "            d_model (int): Dimension of the embedding vectors.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Define the embedding layer that maps token indices to dense vectors.\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)  \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for token embedding.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor of shape (batch_size, seq_len) containing token indices.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of shape (batch_size, seq_len, d_model) containing embedded representations.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the lookup operation using the embedding layer.\n",
    "        embedded = self.embedding(x)  \n",
    "\n",
    "        return embedded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def run_tests():\n",
    "    # Test Parameters\n",
    "    vocab_size = 100\n",
    "    d_model = 16\n",
    "    batch_size = 4\n",
    "    seq_len = 10\n",
    "\n",
    "    # Create a sample input tensor\n",
    "    test_input = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "    # Initialize TokenEmbedding\n",
    "    embedding_layer = TokenEmbedding(vocab_size, d_model)\n",
    "\n",
    "    # Test 1: Check Output Shape\n",
    "    output = embedding_layer(test_input)\n",
    "    assert output.shape == (batch_size, seq_len, d_model), f\"Unexpected shape: {output.shape}\"\n",
    "    \n",
    "    # Test 2: Ensure Output is a Tensor of Correct Type\n",
    "    assert isinstance(output, torch.Tensor), \"Output is not a tensor\"\n",
    "    assert output.dtype == torch.float32, f\"Unexpected dtype: {output.dtype}\"\n",
    "    \n",
    "    # Test 3: Check if the Same Token Index Maps to the Same Embedding\n",
    "    index = torch.tensor([[5]])\n",
    "    embedding_1 = embedding_layer(index)\n",
    "    embedding_2 = embedding_layer(index)\n",
    "    assert torch.allclose(embedding_1, embedding_2), \"Embeddings should be identical for the same index\"\n",
    "    \n",
    "    # Test 4: Check if Different Indices Give Different Embeddings\n",
    "    index1 = torch.tensor([[5]])\n",
    "    index2 = torch.tensor([[8]])\n",
    "    embedding_1 = embedding_layer(index1)\n",
    "    embedding_2 = embedding_layer(index2)\n",
    "    assert not torch.allclose(embedding_1, embedding_2), \"Different indices should have different embeddings\"\n",
    "    \n",
    "    # Test 5: Check if Gradients are Computed\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    assert embedding_layer.embedding.weight.grad is not None, \"Gradients should not be None\"\n",
    "    assert embedding_layer.embedding.weight.grad.shape == (vocab_size, d_model), \"Gradient shape mismatch\"\n",
    "    \n",
    "    print(\"✅ All tests passed successfully!\")\n",
    "\n",
    "# Run all tests\n",
    "run_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5409, -0.4400,  0.0846], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = TokenEmbedding(vocab_size=10, d_model=3)\n",
    "embedding_layer(torch.tensor(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # Create a (max_len, d_model) tensor to hold the positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)            # shape: (max_len, d_model)\n",
    "        \n",
    "        # position: shape (max_len, 1)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # div_term: shape (d_model/2,)  -> we’ll use it for the even/odd splits\n",
    "        # This follows exp(- log(10000) * (2i/d_model)) = 10000^(-2i/d_model).\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        # Apply sine to even indices (0, 2, 4, ...)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # Apply cosine to odd indices (1, 3, 5, ...)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register 'pe' as a buffer so it's not trained\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: shape (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # Grab up to seq_len positions from pe and add to x\n",
    "        # shape of pe_slice becomes (1, seq_len, d_model)\n",
    "        pe_slice = self.pe[:seq_len, :].unsqueeze(0).to(x.device)\n",
    "\n",
    "        return x + pe_slice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All positional encoding tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def run_positional_encoding_tests():\n",
    "    d_model = 16\n",
    "    seq_len = 10\n",
    "    batch_size = 4\n",
    "\n",
    "    test_input = torch.zeros((batch_size, seq_len, d_model))  # Placeholder embeddings\n",
    "    pos_encoding = PositionalEncoding(d_model=d_model)\n",
    "\n",
    "    # Test 1: Check Output Shape\n",
    "    output = pos_encoding(test_input)\n",
    "    assert output.shape == (batch_size, seq_len, d_model), f\"Unexpected shape: {output.shape}\"\n",
    "    \n",
    "    # Test 2: Ensure Output is a Tensor of Correct Type\n",
    "    assert isinstance(output, torch.Tensor), \"Output is not a tensor\"\n",
    "    assert output.dtype == torch.float32, f\"Unexpected dtype: {output.dtype}\"\n",
    "    \n",
    "    # Test 3: Check if Positional Encoding is Being Added\n",
    "    assert not torch.allclose(test_input, output), \"Positional encoding is not being added!\"\n",
    "    \n",
    "    # Test 4: Check Device Compatibility\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    test_input = test_input.to(device)\n",
    "    pos_encoding = pos_encoding.to(device)\n",
    "    output = pos_encoding(test_input)\n",
    "    assert output.device == test_input.device, f\"Device mismatch: {output.device} vs {test_input.device}\"\n",
    "    \n",
    "    # Test 5: Check if Encodings are Deterministic\n",
    "    output1 = pos_encoding(test_input)\n",
    "    output2 = pos_encoding(test_input)\n",
    "    assert torch.allclose(output1, output2), \"Positional encoding should be deterministic!\"\n",
    "    \n",
    "    print(\"✅ All positional encoding tests passed successfully!\")\n",
    "\n",
    "# Run all tests\n",
    "run_positional_encoding_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+01],\n",
      "        [2.0869e+01],\n",
      "        [5.5094e+01],\n",
      "        [1.9833e+02],\n",
      "        [1.0751e+03],\n",
      "        [1.0000e+04],\n",
      "        [1.8968e+05],\n",
      "        [9.2131e+06],\n",
      "        [1.5473e+09],\n",
      "        [1.3358e+12],\n",
      "        [1.0000e+16],\n",
      "        [1.2946e+21],\n",
      "        [7.2048e+27]])\n"
     ]
    }
   ],
   "source": [
    "def scratchboard(max_len, d_model):\n",
    "    pe = torch.zeros(size=(max_len, d_model))\n",
    "    positions = torch.arange(max_len).unsqueeze(1)\n",
    "    div_term = 10**4**(2*positions/d_model)\n",
    "    print(div_term)\n",
    "\n",
    "scratchboard(13, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k: int):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k   # for scaling\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,   # (batch_size, num_heads, seq_len, d_k)\n",
    "        key: torch.Tensor,     # (batch_size, num_heads, seq_len, d_k)\n",
    "        value: torch.Tensor,   # (batch_size, num_heads, seq_len, d_v)\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # (1) QK^T\n",
    "        # key.transpose(-2, -1) is shape (batch_size, num_heads, d_k, seq_len)\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        \n",
    "        # (2) Scale by sqrt(d_k)\n",
    "        attention_scores = attention_scores / math.sqrt(self.d_k)\n",
    "\n",
    "        # (3) If mask is provided, set masked positions to -inf\n",
    "        if mask is not None:\n",
    "            # Typically a 1/0 mask is used; we want to fill 0’s with -inf\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # (4) Apply softmax over the last dimension (seq_len of the key)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # (5) Multiply by V\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledDotProductAttention unit test passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def test_scaled_dot_product_attention():\n",
    "    # Make some deterministic random data.\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    batch_size = 2\n",
    "    num_heads = 3\n",
    "    seq_len_q = 4  # length of the query\n",
    "    seq_len_k = 5  # length of the key\n",
    "    d_k = 6        # dimension per head for query/key\n",
    "    d_v = 6        # dimension per head for value\n",
    "\n",
    "    # Create a random ScaledDotProductAttention instance\n",
    "    attention_module = ScaledDotProductAttention(d_k)\n",
    "\n",
    "    # Create random query, key, value\n",
    "    query = torch.randn(batch_size, num_heads, seq_len_q, d_k)\n",
    "    key   = torch.randn(batch_size, num_heads, seq_len_k, d_k)\n",
    "    value = torch.randn(batch_size, num_heads, seq_len_k, d_v)\n",
    "\n",
    "    # (1) Test forward pass without mask\n",
    "    output, attn_weights = attention_module(query, key, value, mask=None)\n",
    "    \n",
    "    #  -- Check output shape = (batch_size, num_heads, seq_len_q, d_v)\n",
    "    assert output.shape == (batch_size, num_heads, seq_len_q, d_v), \\\n",
    "        f\"Output shape mismatch. Got {output.shape}\"\n",
    "    \n",
    "    #  -- Check attention weight shape = (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    assert attn_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k), \\\n",
    "        f\"Attention weights shape mismatch. Got {attn_weights.shape}\"\n",
    "    \n",
    "    #  -- Check attention weights sum to ~1 across last dimension\n",
    "    attn_sum = attn_weights.sum(dim=-1)\n",
    "    assert torch.allclose(attn_sum, torch.ones_like(attn_sum), atol=1e-5), \\\n",
    "        \"Attention weights do not sum to 1 along the last dimension.\"\n",
    "    \n",
    "    # (2) Test forward pass with a mask (e.g., masking out the last two positions)\n",
    "    #     We'll create a mask of shape (batch_size, 1, seq_len_q, seq_len_k).\n",
    "    #     Suppose we only want the first 3 positions of the key unmasked:\n",
    "    mask = torch.ones(batch_size, 1, seq_len_q, seq_len_k)\n",
    "    mask[:, :, :, -2:] = 0  # mask out the last 2 positions\n",
    "    output_masked, attn_weights_masked = attention_module(query, key, value, mask=mask)\n",
    "\n",
    "    #  -- The masked positions in the softmax should drop to near 0\n",
    "    #     We’ll check the last two positions of each attention row in attn_weights_masked\n",
    "    #     are effectively 0 (within a floating tolerance).\n",
    "    masked_positions = attn_weights_masked[..., -2:]  # shape (batch_size, num_heads, seq_len_q, 2)\n",
    "    assert torch.allclose(masked_positions, torch.zeros_like(masked_positions), atol=1e-5), \\\n",
    "        \"Masking does not appear to zero out the last two positions.\"\n",
    "\n",
    "    print(\"ScaledDotProductAttention unit test passed!\")\n",
    "\n",
    "\n",
    "# Example usage in a Jupyter cell:\n",
    "test_scaled_dot_product_attention()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        \"\"\"\n",
    "        Initializes multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimension of the model (input and output size).\n",
    "            num_heads (int): Number of attention heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Ensure d_model is divisible by num_heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "\n",
    "        # TODO: Define linear transformations for query, key, and value\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # Replace with nn.Linear\n",
    "        self.W_k = nn.Linear(d_model, d_model)  # Replace with nn.Linear\n",
    "        self.W_v = nn.Linear(d_model, d_model)  # Replace with nn.Linear\n",
    "\n",
    "        # TODO: Define output projection layer\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # Replace with nn.Linear\n",
    "\n",
    "        # TODO: Define the scaled dot-product attention module\n",
    "        self.attention = ScaledDotProductAttention(self.d_k)  # Replace with ScaledDotProductAttention(self.d_k)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): Shape (batch_size, seq_len, d_model)\n",
    "            key (torch.Tensor): Shape (batch_size, seq_len, d_model)\n",
    "            value (torch.Tensor): Shape (batch_size, seq_len, d_model)\n",
    "            mask (Optional[torch.Tensor]): Shape (batch_size, 1, seq_len, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Shape (batch_size, seq_len, d_model) - Multi-head attention output.\n",
    "        \"\"\"\n",
    "        # TODO: Apply linear transformations to query, key, and value\n",
    "        Q = self.W_q(query)  # Replace with correct transformation\n",
    "        K = self.W_k(key)  # Replace with correct transformation\n",
    "        V = self.W_v(value)  # Replace with correct transformation\n",
    "\n",
    "        # TODO: Reshape Q, K, V for multi-head attention\n",
    "        # Hint: Use `.view()` and `.transpose()` to shape into (batch_size, num_heads, seq_len, d_k)\n",
    "        batch_size, seq_len, _ = query.shape\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # TODO: Apply scaled dot-product attention\n",
    "        output, attention_weights = self.attention(Q, K, V, mask)  # Replace with correct computation\n",
    "\n",
    "        # TODO: Concatenate the heads back and apply final linear transformation\n",
    "        # Current shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        # We first swap num_heads and seq_len\n",
    "        output = output.transpose(1, 2)  # (batch_size, seq_len, num_heads, d_k)\n",
    "        output = output.contiguous().view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        output = self.W_o(output)  # Replace with correct transformation\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Output shape test passed!\n",
      "✅ Tensor type test passed!\n",
      "✅ Deterministic output test passed!\n",
      "✅ Masking test passed!\n",
      "🎉 All MultiHeadAttention tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_multi_head_attention():\n",
    "    \"\"\"\n",
    "    Tests the MultiHeadAttention module for correctness.\n",
    "    \"\"\"\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "    d_model = 16\n",
    "    num_heads = 4\n",
    "\n",
    "    # Initialize test input tensors\n",
    "    query = torch.randn(batch_size, seq_len, d_model)\n",
    "    key = torch.randn(batch_size, seq_len, d_model)\n",
    "    value = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # Initialize multi-head attention module\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    # Run forward pass without a mask\n",
    "    output = mha(query, key, value, mask=None)\n",
    "\n",
    "    # Test 1: Check output shape\n",
    "    assert output.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Unexpected output shape: {output.shape}\"\n",
    "    print(\"✅ Output shape test passed!\")\n",
    "\n",
    "    # Test 2: Ensure output is a tensor\n",
    "    assert isinstance(output, torch.Tensor), \"Output is not a tensor\"\n",
    "    print(\"✅ Tensor type test passed!\")\n",
    "\n",
    "    # Test 3: Check deterministic output for same input\n",
    "    output_2 = mha(query, key, value, mask=None)\n",
    "    assert torch.allclose(output, output_2), \"Output should be deterministic!\"\n",
    "    print(\"✅ Deterministic output test passed!\")\n",
    "\n",
    "    # Test 4: Apply a mask and check if masking works\n",
    "    mask = torch.zeros(batch_size, 1, seq_len, seq_len)\n",
    "    mask[:, :, :, -1] = float('-inf')  # Mask the last token\n",
    "\n",
    "    output_masked = mha(query, key, value, mask=mask)\n",
    "\n",
    "    # Ensure output is still the correct shape\n",
    "    assert output_masked.shape == (batch_size, seq_len, d_model), \\\n",
    "        \"Masked output shape mismatch\"\n",
    "    print(\"✅ Masking test passed!\")\n",
    "\n",
    "    print(\"🎉 All MultiHeadAttention tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_multi_head_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model) - Transformed representations.\n",
    "        \"\"\"\n",
    "        output = self.fc1(x)\n",
    "        output = self.relu(output)\n",
    "        output = self.fc2(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Output shape test passed!\n",
      "✅ Tensor type test passed!\n",
      "✅ ReLU activation test passed!\n",
      "✅ Deterministic output test passed!\n",
      "✅ Gradient computation test passed!\n",
      "🎉 All PositionwiseFeedForward tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_positionwise_feedforward():\n",
    "    \"\"\"\n",
    "    Tests the PositionwiseFeedForward module.\n",
    "    \"\"\"\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "    d_model = 16\n",
    "    d_ff = 32  # Expanded dimension\n",
    "\n",
    "    # Initialize test input tensor (random)\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # Initialize the feed-forward module\n",
    "    ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "\n",
    "    # Run forward pass\n",
    "    output = ffn(x)\n",
    "\n",
    "    # Test 1: Check output shape\n",
    "    assert output.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Unexpected output shape: {output.shape}\"\n",
    "    print(\"✅ Output shape test passed!\")\n",
    "\n",
    "    # Test 2: Ensure output is a tensor\n",
    "    assert isinstance(output, torch.Tensor), \"Output is not a tensor\"\n",
    "    print(\"✅ Tensor type test passed!\")\n",
    "\n",
    "    # Test 3: Ensure ReLU activation is applied\n",
    "    hidden_layer_output = ffn.fc1(x)  # Get pre-ReLU values\n",
    "    assert torch.all((hidden_layer_output > 0) == (ffn.relu(hidden_layer_output) > 0)), \\\n",
    "        \"ReLU activation is not applied correctly\"\n",
    "    print(\"✅ ReLU activation test passed!\")\n",
    "\n",
    "    # Test 4: Check deterministic output for same input\n",
    "    output_2 = ffn(x)\n",
    "    assert torch.allclose(output, output_2), \"Output should be deterministic!\"\n",
    "    print(\"✅ Deterministic output test passed!\")\n",
    "\n",
    "    # Test 5: Check gradients (ensuring backpropagation works)\n",
    "    output.sum().backward()  # Compute gradients\n",
    "    assert ffn.fc1.weight.grad is not None, \"Gradients are not computed for fc1!\"\n",
    "    assert ffn.fc2.weight.grad is not None, \"Gradients are not computed for fc2!\"\n",
    "    print(\"✅ Gradient computation test passed!\")\n",
    "\n",
    "    print(\"🎉 All PositionwiseFeedForward tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_positionwise_feedforward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initializes a single Transformer Encoder Layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The embedding dimension (must be divisible by num_heads).\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Hidden layer size of the feed-forward network.\n",
    "            dropout (float): Dropout rate (default 0.1).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Define multi-head self-attention layer\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)  # Replace with MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # TODO: Define feed-forward network (FFN)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff)  # Replace with a two-layer FFN\n",
    "\n",
    "        # TODO: Define Layer Normalization layers\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model) \n",
    "\n",
    "        # TODO: Define Dropout layers\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for Transformer Encoder Layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "            mask (Optional[torch.Tensor]): Mask for attention (default None).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_len, d_model).\n",
    "        \"\"\"\n",
    "        # TODO: Apply multi-head self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask=mask) \n",
    "\n",
    "        # TODO: Apply residual connection and layer normalization\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "\n",
    "        # TODO: Apply feed-forward network\n",
    "        ffn_output = self.ffn(x) \n",
    "\n",
    "        # TODO: Apply second residual connection and layer normalization\n",
    "        x = self.norm2(x + self.dropout2(ffn_output))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Instantiation test passed!\n",
      "✅ Forward pass (no mask) shape test passed!\n",
      "✅ Forward pass (with mask) shape test passed!\n",
      "✅ ReLU activation test passed!\n",
      "✅ Backward pass (gradient) test passed!\n",
      "All TransformerEncoderLayer tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytest\n",
    "\n",
    "def test_transformer_encoder_layer():\n",
    "    \"\"\"\n",
    "    Basic tests for TransformerEncoderLayer:\n",
    "    1) Instantiation\n",
    "    2) Forward pass shape (with and without masks)\n",
    "    3) Presence of ReLU (if expected in FFN)\n",
    "    4) Gradient backprop\n",
    "    \"\"\"\n",
    "    # -------------------\n",
    "    # Hyperparams\n",
    "    d_model = 32\n",
    "    num_heads = 4\n",
    "    d_ff = 64\n",
    "    dropout = 0.1\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "\n",
    "    # -------------------\n",
    "    # 1) Instantiate layer\n",
    "    try:\n",
    "        encoder_layer = TransformerEncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Instantiation failed with error: {e}\")\n",
    "\n",
    "    print(\"✅ Instantiation test passed!\")\n",
    "\n",
    "    # -------------------\n",
    "    # Prepare dummy input\n",
    "    x = torch.randn(batch_size, seq_len, d_model)  # (B, S, d_model)\n",
    "\n",
    "    # -------------------\n",
    "    # 2) Forward pass (no mask)\n",
    "    try:\n",
    "        output_no_mask = encoder_layer(x)  # no mask\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed without mask: {e}\")\n",
    "\n",
    "    # Check shape\n",
    "    assert output_no_mask.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape {output_no_mask.shape} != {(batch_size, seq_len, d_model)}\"\n",
    "    print(\"✅ Forward pass (no mask) shape test passed!\")\n",
    "\n",
    "    # -------------------\n",
    "    # 3) Forward pass (with mask)\n",
    "    # Example: a binary mask that \"allows\" everything (all 1s). \n",
    "    # Must match the shape your MultiHeadAttention expects, typically (B, 1, S, S).\n",
    "    mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
    "\n",
    "    try:\n",
    "        output_with_mask = encoder_layer(x, mask=mask)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed with mask: {e}\")\n",
    "\n",
    "    # Check shape\n",
    "    assert output_with_mask.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape {output_with_mask.shape} != {(batch_size, seq_len, d_model)}\"\n",
    "    print(\"✅ Forward pass (with mask) shape test passed!\")\n",
    "\n",
    "    # -------------------\n",
    "    # 4) Check for ReLU activation in feed-forward (optional)\n",
    "    #    If you're using a different activation, change accordingly.\n",
    "    found_relu = False\n",
    "    for submodule in encoder_layer.modules():\n",
    "        if isinstance(submodule, nn.ReLU):\n",
    "            found_relu = True\n",
    "            break\n",
    "    assert found_relu, (\n",
    "        \"No ReLU found in TransformerEncoderLayer's FFN (if you expected one). \"\n",
    "        \"If using a different activation, adjust this test.\"\n",
    "    )\n",
    "    print(\"✅ ReLU activation test passed!\")\n",
    "\n",
    "    # -------------------\n",
    "    # 5) Gradient check\n",
    "    #    Ensure we can do a backward pass without errors\n",
    "    output_with_mask.sum().backward()\n",
    "    print(\"✅ Backward pass (gradient) test passed!\")\n",
    "\n",
    "    print(\"All TransformerEncoderLayer tests passed successfully!\")\n",
    "\n",
    "test_transformer_encoder_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module): \n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Initializes a single Transformer Decoder Layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The embedding dimension (must be divisible by num_heads).\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Hidden layer size of the feed-forward network.\n",
    "            dropout (float): Dropout rate (default 0.1).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Define masked multi-head self-attention layer\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # TODO: Define multi-head attention layer for encoder-decoder attention\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)  # Replace with MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # TODO: Define feed-forward network (FFN)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "\n",
    "        # TODO: Define Layer Normalization layers\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model) \n",
    "        self.norm3 = nn.LayerNorm(d_model) \n",
    "\n",
    "        # TODO: Define Dropout layers\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, memory: torch.Tensor, \n",
    "                src_mask: Optional[torch.Tensor] = None, \n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for Transformer Decoder Layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model) (decoder input).\n",
    "            memory (torch.Tensor): Encoder outputs of shape (batch_size, seq_len_enc, d_model).\n",
    "            tgt_mask (Optional[torch.Tensor]): Mask for target self-attention (default None).\n",
    "            src_mask (Optional[torch.Tensor]): Mask for encoder-decoder attention (default None).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_len, d_model).\n",
    "        \"\"\"\n",
    "        # TODO: Apply masked multi-head self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask=tgt_mask) \n",
    "\n",
    "        # TODO: Apply residual connection and layer normalization\n",
    "        x = self.norm1(attn_output + self.dropout1(x))\n",
    "\n",
    "        # TODO: Apply encoder-decoder multi-head attention\n",
    "        attn_output_2, _ = self.enc_dec_attn(memory, memory, x, mask=src_mask)\n",
    "\n",
    "        # TODO: Apply residual connection and layer normalization\n",
    "        x = self.norm2(attn_output_2 + self.dropout2(x))\n",
    "\n",
    "        # TODO: Apply feed-forward network\n",
    "        ffn_output = self.ffn(x)\n",
    "\n",
    "        # TODO: Apply final residual connection and layer normalization\n",
    "        x = self.norm3(ffn_output + self.dropout3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Instantiation test passed!\n",
      "✅ Forward pass (no masks) shape test passed!\n",
      "✅ Forward pass (with masks) shape test passed!\n",
      "✅ ReLU activation test passed!\n",
      "✅ Backward pass (gradient) test passed!\n",
      "All TransformerDecoderLayer tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytest\n",
    "\n",
    "def test_transformer_decoder_layer():\n",
    "    \"\"\"\n",
    "    Basic tests for TransformerDecoderLayer to check:\n",
    "    1) Instantiation without errors\n",
    "    2) Forward pass shape consistency\n",
    "    3) Handling of optional masks\n",
    "    4) Presence of ReLU activation (if expected)\n",
    "    5) Gradient backprop flow\n",
    "    \"\"\"\n",
    "    d_model = 32\n",
    "    num_heads = 4\n",
    "    d_ff = 64\n",
    "    dropout = 0.1\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "\n",
    "    # 1) Instantiate the layer\n",
    "    try:\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Instantiation failed with error: {e}\")\n",
    "\n",
    "    print(\"✅ Instantiation test passed!\")\n",
    "\n",
    "    # 2) Create dummy inputs\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    memory = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # 3) Forward pass shape test without masks\n",
    "    try:\n",
    "        output = decoder_layer(x, memory)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed without masks: {e}\")\n",
    "\n",
    "    # Check shape\n",
    "    assert output.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape {output.shape} != {(batch_size, seq_len, d_model)}\"\n",
    "    print(\"✅ Forward pass (no masks) shape test passed!\")\n",
    "\n",
    "    # 4) Forward pass with random masks\n",
    "    src_mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
    "    tgt_mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
    "    try:\n",
    "        output_masked = decoder_layer(x, memory, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed with masks: {e}\")\n",
    "\n",
    "    # Check shape again\n",
    "    assert output_masked.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape with masks {output_masked.shape} != {(batch_size, seq_len, d_model)}\"\n",
    "    print(\"✅ Forward pass (with masks) shape test passed!\")\n",
    "\n",
    "    # 5) (Optional) Check for ReLU activation in the layer\n",
    "    #    Depending on your exact implementation, you may not have a direct ReLU submodule.\n",
    "    found_relu = False\n",
    "    for mod in decoder_layer.modules():\n",
    "        if isinstance(mod, nn.ReLU):\n",
    "            found_relu = True\n",
    "            break\n",
    "    assert found_relu, \"No ReLU found in the TransformerDecoderLayer (if you expected one)!\"\n",
    "    print(\"✅ ReLU activation test passed!\")\n",
    "\n",
    "    # 6) Quick gradient test\n",
    "    #    Make sure we can do a backward pass without errors\n",
    "    output_sum = output_masked.sum()\n",
    "    try:\n",
    "        output_sum.backward()\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Backward pass failed: {e}\")\n",
    "\n",
    "    print(\"✅ Backward pass (gradient) test passed!\")\n",
    "\n",
    "    print(\"All TransformerDecoderLayer tests passed successfully!\")\n",
    "\n",
    "test_transformer_decoder_layer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initializes a Transformer Encoder consisting of multiple encoder layers.\n",
    "\n",
    "        Args:\n",
    "            num_layers (int): Number of TransformerEncoderLayer layers.\n",
    "            d_model (int): Dimension of embeddings and model size.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Hidden layer size in feed-forward network.\n",
    "            dropout (float): Dropout rate (default 0.1).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Define a stack of TransformerEncoderLayers\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])  \n",
    "\n",
    "        # TODO: Define final layer normalization\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the Transformer Encoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "            mask (Optional[torch.Tensor]): Optional mask for attention.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Encoded representation of shape (batch_size, seq_len, d_model).\n",
    "        \"\"\"\n",
    "        # TODO: Pass input through each TransformerEncoderLayer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        # TODO: Apply final normalization\n",
    "        x = self.norm(x)  \n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Instantiation test passed!\n",
      "✅ Forward pass (no mask) shape test passed!\n",
      "✅ Forward pass (with mask) shape test passed!\n",
      "✅ ReLU activation test passed!\n",
      "✅ Backward pass (gradient) test passed!\n",
      "All TransformerEncoder tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytest\n",
    "\n",
    "def test_transformer_encoder():\n",
    "    \"\"\"\n",
    "    Basic tests for TransformerEncoder to check:\n",
    "    1) Instantiation without errors\n",
    "    2) Forward pass shape consistency (with and without masks)\n",
    "    3) Presence of ReLU activation (if expected)\n",
    "    4) Gradient backprop flow\n",
    "    \"\"\"\n",
    "    # Hyperparameters and dummy inputs\n",
    "    num_layers = 2\n",
    "    d_model = 32\n",
    "    num_heads = 4\n",
    "    d_ff = 64\n",
    "    dropout = 0.1\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "\n",
    "    # 1) Instantiate the encoder\n",
    "    try:\n",
    "        encoder = TransformerEncoder(num_layers, d_model, num_heads, d_ff, dropout)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Instantiation failed with error: {e}\")\n",
    "\n",
    "    print(\"✅ Instantiation test passed!\")\n",
    "\n",
    "    # Create dummy inputs\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # 2) Forward pass shape test without mask\n",
    "    try:\n",
    "        output_no_mask = encoder(x)  # no mask\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed without mask: {e}\")\n",
    "\n",
    "    # Check shape\n",
    "    assert output_no_mask.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape {output_no_mask.shape} != {(batch_size, seq_len, d_model)}\"\n",
    "    print(\"✅ Forward pass (no mask) shape test passed!\")\n",
    "\n",
    "    # 3) Forward pass with a dummy mask\n",
    "    mask = torch.ones(batch_size, 1, seq_len, seq_len)  # e.g., all ones as a placeholder\n",
    "    try:\n",
    "        output_with_mask = encoder(x, mask)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed with mask: {e}\")\n",
    "\n",
    "    assert output_with_mask.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape with mask {output_with_mask.shape} != {(batch_size, seq_len, d_model)}\"\n",
    "    print(\"✅ Forward pass (with mask) shape test passed!\")\n",
    "\n",
    "    # 4) (Optional) Check for ReLU activation in the encoder\n",
    "    #    Adjust this if your encoder uses a different activation function.\n",
    "    found_relu = False\n",
    "    for mod in encoder.modules():\n",
    "        if isinstance(mod, nn.ReLU):\n",
    "            found_relu = True\n",
    "            break\n",
    "    assert found_relu, \"No ReLU found in the TransformerEncoder (if you expected one)!\"\n",
    "    print(\"✅ ReLU activation test passed!\")\n",
    "\n",
    "    # 5) Quick gradient test\n",
    "    #    Make sure backward pass works\n",
    "    output_with_mask.sum().backward()  # should not error out\n",
    "    print(\"✅ Backward pass (gradient) test passed!\")\n",
    "\n",
    "    print(\"All TransformerEncoder tests passed successfully!\")\n",
    "\n",
    "test_transformer_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initializes a Transformer Decoder consisting of multiple decoder layers.\n",
    "\n",
    "        Args:\n",
    "            num_layers (int): Number of TransformerDecoderLayer layers.\n",
    "            d_model (int): Dimension of embeddings and model size.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Hidden layer size in feed-forward network.\n",
    "            dropout (float): Dropout rate (default 0.1).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Define a stack of TransformerDecoderLayers\n",
    "        self.layers = nn.ModuleList([TransformerDecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])  # Replace with nn.ModuleList([...])\n",
    "\n",
    "        # TODO: Define final layer normalization\n",
    "        self.norm = nn.LayerNorm(d_model)  # Replace with nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, memory: torch.Tensor, \n",
    "                tgt_mask: Optional[torch.Tensor] = None, \n",
    "                src_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the Transformer Decoder.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Decoder input tensor (batch_size, tgt_seq_len, d_model).\n",
    "            memory (torch.Tensor): Encoder outputs (batch_size, src_seq_len, d_model).\n",
    "            tgt_mask (Optional[torch.Tensor]): Mask for target self-attention.\n",
    "            src_mask (Optional[torch.Tensor]): Mask for encoder-decoder attention.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Decoded representation of shape (batch_size, tgt_seq_len, d_model).\n",
    "        \"\"\"\n",
    "        # TODO: Pass input through each TransformerDecoderLayer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, tgt_mask, src_mask)  # Replace with layer(x, memory, tgt_mask, src_mask)\n",
    "\n",
    "        # TODO: Apply final normalization\n",
    "        x = self.norm(x)  # Replace with self.norm(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Instantiation test passed!\n",
      "✅ Forward pass (no masks) shape test passed!\n",
      "✅ Forward pass (with masks) shape test passed!\n",
      "✅ ReLU activation test passed!\n",
      "✅ Backward pass (gradient) test passed!\n",
      "All TransformerDecoder tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytest\n",
    "\n",
    "def test_transformer_decoder():\n",
    "    \"\"\"\n",
    "    Basic tests for TransformerDecoder to check:\n",
    "    1) Instantiation\n",
    "    2) Forward pass shape consistency (with and without masks)\n",
    "    3) Presence of ReLU activation (if expected)\n",
    "    4) Gradient backprop\n",
    "    \"\"\"\n",
    "\n",
    "    # Hyperparameters & dummy input sizes\n",
    "    num_layers = 2\n",
    "    d_model = 32\n",
    "    num_heads = 4\n",
    "    d_ff = 64\n",
    "    dropout = 0.1\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "\n",
    "    # 1) Instantiate the decoder\n",
    "    try:\n",
    "        decoder = TransformerDecoder(num_layers, d_model, num_heads, d_ff, dropout)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Instantiation failed with error: {e}\")\n",
    "\n",
    "    print(\"✅ Instantiation test passed!\")\n",
    "\n",
    "    # Create dummy inputs\n",
    "    x = torch.randn(batch_size, seq_len, d_model)      # Target sequence\n",
    "    memory = torch.randn(batch_size, seq_len, d_model) # Encoder output\n",
    "\n",
    "    # 2) Forward pass without masks\n",
    "    try:\n",
    "        output_no_mask = decoder(x, memory)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed without masks: {e}\")\n",
    "\n",
    "    # Check shape\n",
    "    assert output_no_mask.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape {output_no_mask.shape} != {(batch_size, seq_len, d_model)}\"\n",
    "    print(\"✅ Forward pass (no masks) shape test passed!\")\n",
    "\n",
    "    # 3) Forward pass with masks\n",
    "    src_mask = torch.ones(batch_size, 1, seq_len, seq_len)  # dummy encoder mask\n",
    "    tgt_mask = torch.ones(batch_size, 1, seq_len, seq_len)  # dummy decoder mask\n",
    "    try:\n",
    "        output_with_masks = decoder(x, memory, src_mask, tgt_mask)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed with masks: {e}\")\n",
    "\n",
    "    # Check shape again\n",
    "    assert output_with_masks.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape with masks {output_with_masks.shape} != {(batch_size, seq_len, d_model)}\"\n",
    "    print(\"✅ Forward pass (with masks) shape test passed!\")\n",
    "\n",
    "    # 4) (Optional) Check for ReLU activation in the decoder\n",
    "    #    Adjust this if your decoder uses a different activation\n",
    "    found_relu = False\n",
    "    for mod in decoder.modules():\n",
    "        if isinstance(mod, nn.ReLU):\n",
    "            found_relu = True\n",
    "            break\n",
    "    assert found_relu, \"No ReLU found in the TransformerDecoder (if you expected one)!\"\n",
    "    print(\"✅ ReLU activation test passed!\")\n",
    "\n",
    "    # 5) Quick gradient test\n",
    "    #    Make sure backward pass works\n",
    "    output_with_masks.sum().backward()  # Should not raise an error\n",
    "    print(\"✅ Backward pass (gradient) test passed!\")\n",
    "\n",
    "    print(\"All TransformerDecoder tests passed successfully!\")\n",
    "    \n",
    "test_transformer_decoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, num_layers: int, num_heads: int, \n",
    "                 d_ff: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Number of unique tokens in the vocabulary.\n",
    "            d_model (int): Embedding dimension.\n",
    "            num_layers (int): Number of encoder and decoder layers.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Hidden layer size in feed-forward network.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Define token embeddings\n",
    "        self.embedding = None  # Replace with nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # TODO: Define positional encoding\n",
    "        self.pos_encoding = None  # Replace with PositionalEncoding(d_model)\n",
    "\n",
    "        # TODO: Define the encoder\n",
    "        self.encoder = None  # Replace with TransformerEncoder(...)\n",
    "\n",
    "        # TODO: Define the decoder\n",
    "        self.decoder = None  # Replace with TransformerDecoder(...)\n",
    "\n",
    "        # TODO: Define the final projection layer\n",
    "        self.fc_out = None  # Replace with nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, \n",
    "                src_mask: Optional[torch.Tensor] = None, \n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the Transformer model.\n",
    "\n",
    "        Args:\n",
    "            src (torch.Tensor): Source token indices of shape (batch_size, src_seq_len).\n",
    "            tgt (torch.Tensor): Target token indices of shape (batch_size, tgt_seq_len).\n",
    "            src_mask (Optional[torch.Tensor]): Source mask of shape (batch_size, 1, src_seq_len, src_seq_len).\n",
    "            tgt_mask (Optional[torch.Tensor]): Target mask of shape (batch_size, 1, tgt_seq_len, tgt_seq_len).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Token probabilities of shape (batch_size, tgt_seq_len, vocab_size).\n",
    "        \"\"\"\n",
    "        # TODO: Apply token embedding and positional encoding\n",
    "        src_emb = None  # Replace with self.embedding(src) + self.pos_encoding(src)\n",
    "        tgt_emb = None  # Replace with self.embedding(tgt) + self.pos_encoding(tgt)\n",
    "\n",
    "        # TODO: Pass through the encoder\n",
    "        memory = None  # Replace with self.encoder(src_emb, src_mask)\n",
    "\n",
    "        # TODO: Pass through the decoder\n",
    "        output = None  # Replace with self.decoder(tgt_emb, memory, tgt_mask, src_mask)\n",
    "\n",
    "        # TODO: Apply final linear layer to project into vocab size\n",
    "        logits = None  # Replace with self.fc_out(output)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Instantiation test passed!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 96\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Backward pass (gradient) test passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll Transformer tests passed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m \u001b[43mtest_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 52\u001b[0m, in \u001b[0;36mtest_transformer\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Check shape: (batch_size, tgt_seq_len, vocab_size)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m expected_shape \u001b[38;5;241m=\u001b[39m (batch_size, tgt_seq_len, vocab_size)\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43moutput_no_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m \u001b[38;5;241m==\u001b[39m expected_shape, \\\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_no_mask\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Forward pass (no masks) shape test passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# 3) Forward pass with masks\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Example: random binary masks (1 = keep, 0 = mask). \u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Some implementations expect float masks with -inf for masked positions. \u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Adjust as needed for your attention mechanism.\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytest\n",
    "\n",
    "def test_transformer():\n",
    "    \"\"\"\n",
    "    Basic tests for the Transformer model:\n",
    "    1) Instantiation\n",
    "    2) Forward pass shape (with and without masks)\n",
    "    3) Presence of ReLU (if expected)\n",
    "    4) Gradient backprop\n",
    "    \"\"\"\n",
    "    # Hyperparameters\n",
    "    vocab_size = 10\n",
    "    d_model = 8\n",
    "    num_layers = 2\n",
    "    num_heads = 2\n",
    "    d_ff = 16\n",
    "    dropout = 0.1\n",
    "\n",
    "    batch_size = 2\n",
    "    src_seq_len = 5\n",
    "    tgt_seq_len = 6\n",
    "\n",
    "    # 1) Instantiate\n",
    "    try:\n",
    "        model = Transformer(\n",
    "            vocab_size=vocab_size,\n",
    "            d_model=d_model,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            d_ff=d_ff,\n",
    "            dropout=dropout\n",
    "        )\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Instantiation failed with error: {e}\")\n",
    "\n",
    "    print(\"✅ Instantiation test passed!\")\n",
    "\n",
    "    # Create dummy source & target inputs (token indices)\n",
    "    src = torch.randint(0, vocab_size, (batch_size, src_seq_len))\n",
    "    tgt = torch.randint(0, vocab_size, (batch_size, tgt_seq_len))\n",
    "\n",
    "    # 2) Forward pass without masks\n",
    "    try:\n",
    "        output_no_mask = model(src, tgt)  # no masks\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed without masks: {e}\")\n",
    "\n",
    "    # Check shape: (batch_size, tgt_seq_len, vocab_size)\n",
    "    expected_shape = (batch_size, tgt_seq_len, vocab_size)\n",
    "    assert output_no_mask.shape == expected_shape, \\\n",
    "        f\"Output shape {output_no_mask.shape} != {expected_shape}\"\n",
    "\n",
    "    print(\"✅ Forward pass (no masks) shape test passed!\")\n",
    "\n",
    "    # 3) Forward pass with masks\n",
    "    # Example: random binary masks (1 = keep, 0 = mask). \n",
    "    # Some implementations expect float masks with -inf for masked positions. \n",
    "    # Adjust as needed for your attention mechanism.\n",
    "    src_mask = torch.ones(batch_size, 1, src_seq_len, src_seq_len)\n",
    "    tgt_mask = torch.ones(batch_size, 1, tgt_seq_len, tgt_seq_len)\n",
    "\n",
    "    try:\n",
    "        output_with_masks = model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed with masks: {e}\")\n",
    "\n",
    "    assert output_with_masks.shape == expected_shape, \\\n",
    "        f\"Output shape with masks {output_with_masks.shape} != {expected_shape}\"\n",
    "\n",
    "    print(\"✅ Forward pass (with masks) shape test passed!\")\n",
    "\n",
    "    # 4) Check for ReLU activation in the model (optional)\n",
    "    #    If you use a different activation like GELU, adjust this accordingly.\n",
    "    found_relu = False\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.ReLU):\n",
    "            found_relu = True\n",
    "            break\n",
    "    assert found_relu, \"No ReLU found in Transformer (if you expected it)!\"\n",
    "    print(\"✅ ReLU activation test passed!\")\n",
    "\n",
    "    # 5) Gradient backprop check\n",
    "    #    Make sure we can run backward without errors\n",
    "    loss = output_with_masks.sum()  # Simple scalar\n",
    "    try:\n",
    "        loss.backward()\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Backward pass failed: {e}\")\n",
    "\n",
    "    print(\"✅ Backward pass (gradient) test passed!\")\n",
    "\n",
    "    print(\"All Transformer tests passed successfully!\")\n",
    "\n",
    "test_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerTrainer:\n",
    "    def __init__(self, model: Transformer, learning_rate: float, weight_decay: float):\n",
    "        \"\"\"\n",
    "        Initializes optimizer and loss function for training.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def train_step(self, src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Runs a single training step.\n",
    "\n",
    "        Args:\n",
    "            src: (batch_size, src_seq_len) - Source sequence.\n",
    "            tgt: (batch_size, tgt_seq_len) - Target sequence.\n",
    "\n",
    "        Returns:\n",
    "            Loss value.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, src: torch.Tensor, tgt: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Evaluates the model on a validation set.\n",
    "\n",
    "        Args:\n",
    "            src: (batch_size, src_seq_len) - Source sequence.\n",
    "            tgt: (batch_size, tgt_seq_len) - Target sequence.\n",
    "\n",
    "        Returns:\n",
    "            BLEU score or another evaluation metric.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention-is-all-you-need",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
