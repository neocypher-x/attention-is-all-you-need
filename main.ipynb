{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up an Anaconda environment for implementing the Transformer model in PyTorch, follow these steps:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Create a New Conda Environment**\n",
    "Open a terminal and run:\n",
    "```bash\n",
    "conda create --name attention-is-all-you-need python=3.12\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Activate the Environment**\n",
    "```bash\n",
    "conda activate attention-is-all-you-need\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Install PyTorch**\n",
    "For GPU (CUDA):\n",
    "```bash\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "```\n",
    "For CPU (if you don’t have a compatible GPU):\n",
    "```bash\n",
    "conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
    "```\n",
    "Check if PyTorch is installed correctly:\n",
    "```python\n",
    "python -c \"import torch; print(torch.__version__)\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Install Essential Libraries**\n",
    "```bash\n",
    "pip install numpy pandas matplotlib tqdm\n",
    "```\n",
    "- `numpy`: Tensor operations\n",
    "- `pandas`: Data handling (optional, useful for datasets)\n",
    "- `matplotlib`: Visualization\n",
    "- `tqdm`: Progress bars for training\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Install NLP Libraries (If Needed)**\n",
    "```bash\n",
    "pip install transformers datasets tokenizers sentencepiece\n",
    "```\n",
    "- `transformers`: Pretrained models from Hugging Face (optional)\n",
    "- `datasets`: NLP datasets from Hugging Face\n",
    "- `tokenizers`: Efficient tokenization\n",
    "- `sentencepiece`: Subword tokenization (used in original Transformer)\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Install Jupyter Notebook (Optional)**\n",
    "If you want to develop in Jupyter:\n",
    "```bash\n",
    "conda install jupyter\n",
    "```\n",
    "Then start Jupyter:\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Verify Everything**\n",
    "Run the following to ensure your environment is properly set up:\n",
    "```python\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Save the Environment (Optional)**\n",
    "To export your environment for reproducibility:\n",
    "```bash\n",
    "conda env export > environment.yml\n",
    "```\n",
    "To recreate it later:\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int):\n",
    "        \"\"\"\n",
    "        Initializes the embedding layer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Number of unique tokens in the vocabulary.\n",
    "            d_model (int): Dimension of the embedding vectors.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Define the embedding layer that maps token indices to dense vectors.\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)  \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for token embedding.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor of shape (batch_size, seq_len) containing token indices.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of shape (batch_size, seq_len, d_model) containing embedded representations.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the lookup operation using the embedding layer.\n",
    "        embedded = self.embedding(x)  \n",
    "\n",
    "        return embedded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def run_tests():\n",
    "    # Test Parameters\n",
    "    vocab_size = 100\n",
    "    d_model = 16\n",
    "    batch_size = 4\n",
    "    seq_len = 10\n",
    "\n",
    "    # Create a sample input tensor\n",
    "    test_input = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "    # Initialize TokenEmbedding\n",
    "    embedding_layer = TokenEmbedding(vocab_size, d_model)\n",
    "\n",
    "    # Test 1: Check Output Shape\n",
    "    output = embedding_layer(test_input)\n",
    "    assert output.shape == (batch_size, seq_len, d_model), f\"Unexpected shape: {output.shape}\"\n",
    "    \n",
    "    # Test 2: Ensure Output is a Tensor of Correct Type\n",
    "    assert isinstance(output, torch.Tensor), \"Output is not a tensor\"\n",
    "    assert output.dtype == torch.float32, f\"Unexpected dtype: {output.dtype}\"\n",
    "    \n",
    "    # Test 3: Check if the Same Token Index Maps to the Same Embedding\n",
    "    index = torch.tensor([[5]])\n",
    "    embedding_1 = embedding_layer(index)\n",
    "    embedding_2 = embedding_layer(index)\n",
    "    assert torch.allclose(embedding_1, embedding_2), \"Embeddings should be identical for the same index\"\n",
    "    \n",
    "    # Test 4: Check if Different Indices Give Different Embeddings\n",
    "    index1 = torch.tensor([[5]])\n",
    "    index2 = torch.tensor([[8]])\n",
    "    embedding_1 = embedding_layer(index1)\n",
    "    embedding_2 = embedding_layer(index2)\n",
    "    assert not torch.allclose(embedding_1, embedding_2), \"Different indices should have different embeddings\"\n",
    "    \n",
    "    # Test 5: Check if Gradients are Computed\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    assert embedding_layer.embedding.weight.grad is not None, \"Gradients should not be None\"\n",
    "    assert embedding_layer.embedding.weight.grad.shape == (vocab_size, d_model), \"Gradient shape mismatch\"\n",
    "    \n",
    "    print(\"✅ All tests passed successfully!\")\n",
    "\n",
    "# Run all tests\n",
    "run_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9214,  0.2546, -1.1575], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = TokenEmbedding(vocab_size=10, d_model=3)\n",
    "embedding_layer(torch.tensor(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import math\n",
    "\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model: int, max_len: int = 5000):\n",
    "#         \"\"\"\n",
    "#         Initializes positional encoding.\n",
    "\n",
    "#         Args:\n",
    "#             d_model (int): Dimension of the embedding vectors.\n",
    "#             max_len (int): Maximum sequence length.\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "\n",
    "#         # TODO: Create an empty tensor to hold positional encodings of shape (max_len, d_model)\n",
    "#         pe = torch.zeros(size=(max_len, d_model))\n",
    "\n",
    "#         # TODO: Create a position index tensor of shape (max_len, 1)\n",
    "#         positions = torch.arange(max_len).unsqueeze(1)  # Replace with the correct initialization\n",
    "\n",
    "#         # TODO: Compute the denominator term for the sine/cosine functions\n",
    "#         # div_term = 10**4**(2*positions/d_model)  # Replace with the correct computation\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "#         # TODO: Compute sin and cos positional encodings\n",
    "#         # Apply sine to even indices and cosine to odd indices\n",
    "#         # Hint: Use slicing `self.pe[:, 0::2] = ...` for even indices\n",
    "#         #       Use slicing `self.pe[:, 1::2] = ...` for odd indices\n",
    "#         pe[:, 0::2] = torch.sin(positions/div_term)\n",
    "#         pe[:, 1::2] = torch.cos(positions/div_term)\n",
    "\n",
    "#         # TODO: Register `self.pe` as a buffer so it doesn't update during training\n",
    "#         # Hint: Use `self.register_buffer(\"pe\", self.pe)`\n",
    "#         self.register_buffer(\"pe\", pe)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         \"\"\"\n",
    "#         Adds positional encoding to the input embeddings.\n",
    "\n",
    "#         Args:\n",
    "#             x (torch.Tensor): Tensor of shape (batch_size, seq_len, d_model) containing input embeddings.\n",
    "\n",
    "#         Returns:\n",
    "#             torch.Tensor: Tensor of shape (batch_size, seq_len, d_model) with positional encodings added.\n",
    "#         \"\"\"\n",
    "#         # TODO: Retrieve only the necessary positions up to the input sequence length\n",
    "#         # Hint: Slice `self.pe` correctly based on `x.size(1)`\n",
    "#         pe_slice = self.pe[:x.size(1),:].unsqueeze(0)\n",
    "\n",
    "#         # TODO: Add positional encodings to the input embeddings\n",
    "#         # Hint: Ensure the positional encodings are on the same device as `x`\n",
    "#         pe_slice.to(x.device)\n",
    "\n",
    "#         return x + pe_slice  # Replace with the final tensor with positional encoding added\n",
    "    \n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # Create a (max_len, d_model) tensor to hold the positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)            # shape: (max_len, d_model)\n",
    "        \n",
    "        # position: shape (max_len, 1)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # div_term: shape (d_model/2,)  -> we’ll use it for the even/odd splits\n",
    "        # This follows exp(- log(10000) * (2i/d_model)) = 10000^(-2i/d_model).\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        # Apply sine to even indices (0, 2, 4, ...)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # Apply cosine to odd indices (1, 3, 5, ...)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register 'pe' as a buffer so it's not trained\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: shape (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # Grab up to seq_len positions from pe and add to x\n",
    "        # shape of pe_slice becomes (1, seq_len, d_model)\n",
    "        pe_slice = self.pe[:seq_len, :].unsqueeze(0).to(x.device)\n",
    "\n",
    "        return x + pe_slice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All positional encoding tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def run_positional_encoding_tests():\n",
    "    d_model = 16\n",
    "    seq_len = 10\n",
    "    batch_size = 4\n",
    "\n",
    "    test_input = torch.zeros((batch_size, seq_len, d_model))  # Placeholder embeddings\n",
    "    pos_encoding = PositionalEncoding(d_model=d_model)\n",
    "\n",
    "    # Test 1: Check Output Shape\n",
    "    output = pos_encoding(test_input)\n",
    "    assert output.shape == (batch_size, seq_len, d_model), f\"Unexpected shape: {output.shape}\"\n",
    "    \n",
    "    # Test 2: Ensure Output is a Tensor of Correct Type\n",
    "    assert isinstance(output, torch.Tensor), \"Output is not a tensor\"\n",
    "    assert output.dtype == torch.float32, f\"Unexpected dtype: {output.dtype}\"\n",
    "    \n",
    "    # Test 3: Check if Positional Encoding is Being Added\n",
    "    assert not torch.allclose(test_input, output), \"Positional encoding is not being added!\"\n",
    "    \n",
    "    # Test 4: Check Device Compatibility\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    test_input = test_input.to(device)\n",
    "    pos_encoding = pos_encoding.to(device)\n",
    "    output = pos_encoding(test_input)\n",
    "    assert output.device == test_input.device, f\"Device mismatch: {output.device} vs {test_input.device}\"\n",
    "    \n",
    "    # Test 5: Check if Encodings are Deterministic\n",
    "    output1 = pos_encoding(test_input)\n",
    "    output2 = pos_encoding(test_input)\n",
    "    assert torch.allclose(output1, output2), \"Positional encoding should be deterministic!\"\n",
    "    \n",
    "    print(\"✅ All positional encoding tests passed successfully!\")\n",
    "\n",
    "# Run all tests\n",
    "run_positional_encoding_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+01],\n",
      "        [2.0869e+01],\n",
      "        [5.5094e+01],\n",
      "        [1.9833e+02],\n",
      "        [1.0751e+03],\n",
      "        [1.0000e+04],\n",
      "        [1.8968e+05],\n",
      "        [9.2131e+06],\n",
      "        [1.5473e+09],\n",
      "        [1.3358e+12],\n",
      "        [1.0000e+16],\n",
      "        [1.2946e+21],\n",
      "        [7.2048e+27]])\n"
     ]
    }
   ],
   "source": [
    "def scratchboard(max_len, d_model):\n",
    "    pe = torch.zeros(size=(max_len, d_model))\n",
    "    positions = torch.arange(max_len).unsqueeze(1)\n",
    "    div_term = 10**4**(2*positions/d_model)\n",
    "    print(div_term)\n",
    "\n",
    "scratchboard(13, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from typing import Optional, Tuple\n",
    "\n",
    "# class ScaledDotProductAttention(nn.Module):\n",
    "#     def __init__(self, d_k: int):\n",
    "#         \"\"\"\n",
    "#         Initializes scaled dot-product attention.\n",
    "\n",
    "#         Args:\n",
    "#             d_k (int): Dimension of the key vectors (used for scaling).\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "\n",
    "#         # TODO: Store d_k for scaling attention scores\n",
    "#         self.d_k = d_k  # Replace with correct initialization\n",
    "\n",
    "#     def forward(\n",
    "#         self, \n",
    "#         query: torch.Tensor, \n",
    "#         key: torch.Tensor, \n",
    "#         value: torch.Tensor, \n",
    "#         mask: Optional[torch.Tensor] = None\n",
    "#     ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "#         \"\"\"\n",
    "#         Computes the scaled dot-product attention.\n",
    "\n",
    "#         Args:\n",
    "#             query (torch.Tensor): Shape (batch_size, num_heads, seq_len, d_k)\n",
    "#             key (torch.Tensor): Shape (batch_size, num_heads, seq_len, d_k)\n",
    "#             value (torch.Tensor): Shape (batch_size, num_heads, seq_len, d_v)\n",
    "#             mask (Optional[torch.Tensor]): Shape (batch_size, 1, seq_len, seq_len) \n",
    "#                                            (mask for padding or future tokens in decoder)\n",
    "\n",
    "#         Returns:\n",
    "#             Tuple[torch.Tensor, torch.Tensor]: \n",
    "#                 - Attention output of shape (batch_size, num_heads, seq_len, d_v)\n",
    "#                 - Attention weights of shape (batch_size, num_heads, seq_len, seq_len)\n",
    "#         \"\"\"\n",
    "#         # TODO: Compute attention scores as QK^T / sqrt(d_k)\n",
    "#         attention_scores = torch.matmul(query, torch.transpose(key, -2, -1)) / math.sqrt(self.d_k)   # Replace with correct computation\n",
    "\n",
    "#         # TODO: Apply mask (if provided) by setting masked positions to a very low value\n",
    "#         # Hint: Use `float('-inf')` for masked positions before applying softmax\n",
    "#         if mask is not None:\n",
    "#             attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "#         # TODO: Compute attention weights using softmax\n",
    "#         attention_weights = torch.softmax(attention_scores, dim=-1)  # Replace with correct computation\n",
    "\n",
    "#         # TODO: Multiply attention weights by value matrix to get the final output\n",
    "#         output = torch.matmul(attention_weights, value)  # Replace with correct computation\n",
    "\n",
    "#         return output, attention_weights  # Return attention output and weights\n",
    "    \n",
    "\n",
    "import math\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k: int):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k   # for scaling\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,   # (batch_size, num_heads, seq_len, d_k)\n",
    "        key: torch.Tensor,     # (batch_size, num_heads, seq_len, d_k)\n",
    "        value: torch.Tensor,   # (batch_size, num_heads, seq_len, d_v)\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # (1) QK^T\n",
    "        # key.transpose(-2, -1) is shape (batch_size, num_heads, d_k, seq_len)\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        \n",
    "        # (2) Scale by sqrt(d_k)\n",
    "        attention_scores = attention_scores / math.sqrt(self.d_k)\n",
    "\n",
    "        # (3) If mask is provided, set masked positions to -inf\n",
    "        if mask is not None:\n",
    "            # Typically a 1/0 mask is used; we want to fill 0’s with -inf\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # (4) Apply softmax over the last dimension (seq_len of the key)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # (5) Multiply by V\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledDotProductAttention unit test passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def test_scaled_dot_product_attention():\n",
    "    # Make some deterministic random data.\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    batch_size = 2\n",
    "    num_heads = 3\n",
    "    seq_len_q = 4  # length of the query\n",
    "    seq_len_k = 5  # length of the key\n",
    "    d_k = 6        # dimension per head for query/key\n",
    "    d_v = 6        # dimension per head for value\n",
    "\n",
    "    # Create a random ScaledDotProductAttention instance\n",
    "    attention_module = ScaledDotProductAttention(d_k)\n",
    "\n",
    "    # Create random query, key, value\n",
    "    query = torch.randn(batch_size, num_heads, seq_len_q, d_k)\n",
    "    key   = torch.randn(batch_size, num_heads, seq_len_k, d_k)\n",
    "    value = torch.randn(batch_size, num_heads, seq_len_k, d_v)\n",
    "\n",
    "    # (1) Test forward pass without mask\n",
    "    output, attn_weights = attention_module(query, key, value, mask=None)\n",
    "    \n",
    "    #  -- Check output shape = (batch_size, num_heads, seq_len_q, d_v)\n",
    "    assert output.shape == (batch_size, num_heads, seq_len_q, d_v), \\\n",
    "        f\"Output shape mismatch. Got {output.shape}\"\n",
    "    \n",
    "    #  -- Check attention weight shape = (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    assert attn_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k), \\\n",
    "        f\"Attention weights shape mismatch. Got {attn_weights.shape}\"\n",
    "    \n",
    "    #  -- Check attention weights sum to ~1 across last dimension\n",
    "    attn_sum = attn_weights.sum(dim=-1)\n",
    "    assert torch.allclose(attn_sum, torch.ones_like(attn_sum), atol=1e-5), \\\n",
    "        \"Attention weights do not sum to 1 along the last dimension.\"\n",
    "    \n",
    "    # (2) Test forward pass with a mask (e.g., masking out the last two positions)\n",
    "    #     We'll create a mask of shape (batch_size, 1, seq_len_q, seq_len_k).\n",
    "    #     Suppose we only want the first 3 positions of the key unmasked:\n",
    "    mask = torch.ones(batch_size, 1, seq_len_q, seq_len_k)\n",
    "    mask[:, :, :, -2:] = 0  # mask out the last 2 positions\n",
    "    output_masked, attn_weights_masked = attention_module(query, key, value, mask=mask)\n",
    "\n",
    "    #  -- The masked positions in the softmax should drop to near 0\n",
    "    #     We’ll check the last two positions of each attention row in attn_weights_masked\n",
    "    #     are effectively 0 (within a floating tolerance).\n",
    "    masked_positions = attn_weights_masked[..., -2:]  # shape (batch_size, num_heads, seq_len_q, 2)\n",
    "    assert torch.allclose(masked_positions, torch.zeros_like(masked_positions), atol=1e-5), \\\n",
    "        \"Masking does not appear to zero out the last two positions.\"\n",
    "\n",
    "    print(\"ScaledDotProductAttention unit test passed!\")\n",
    "\n",
    "\n",
    "# Example usage in a Jupyter cell:\n",
    "test_scaled_dot_product_attention()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        \"\"\"\n",
    "        Initializes multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimension of the model (input and output size).\n",
    "            num_heads (int): Number of attention heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Ensure d_model is divisible by num_heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "\n",
    "        # TODO: Define linear transformations for query, key, and value\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # Replace with nn.Linear\n",
    "        self.W_k = nn.Linear(d_model, d_model)  # Replace with nn.Linear\n",
    "        self.W_v = nn.Linear(d_model, d_model)  # Replace with nn.Linear\n",
    "\n",
    "        # TODO: Define output projection layer\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # Replace with nn.Linear\n",
    "\n",
    "        # TODO: Define the scaled dot-product attention module\n",
    "        self.attention = ScaledDotProductAttention(self.d_k)  # Replace with ScaledDotProductAttention(self.d_k)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): Shape (batch_size, seq_len, d_model)\n",
    "            key (torch.Tensor): Shape (batch_size, seq_len, d_model)\n",
    "            value (torch.Tensor): Shape (batch_size, seq_len, d_model)\n",
    "            mask (Optional[torch.Tensor]): Shape (batch_size, 1, seq_len, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Shape (batch_size, seq_len, d_model) - Multi-head attention output.\n",
    "        \"\"\"\n",
    "        # TODO: Apply linear transformations to query, key, and value\n",
    "        Q = self.W_q(query)  # Replace with correct transformation\n",
    "        K = self.W_k(key)  # Replace with correct transformation\n",
    "        V = self.W_v(value)  # Replace with correct transformation\n",
    "\n",
    "        # TODO: Reshape Q, K, V for multi-head attention\n",
    "        # Hint: Use `.view()` and `.transpose()` to shape into (batch_size, num_heads, seq_len, d_k)\n",
    "        batch_size, seq_len, _ = query.shape\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # TODO: Apply scaled dot-product attention\n",
    "        output, attention_weights = self.attention(Q, K, V, mask)  # Replace with correct computation\n",
    "\n",
    "        # TODO: Concatenate the heads back and apply final linear transformation\n",
    "        # Current shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        # We first swap num_heads and seq_len\n",
    "        output = output.transpose(1, 2)  # (batch_size, seq_len, num_heads, d_k)\n",
    "        output = output.contiguous().view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        output = self.W_o(output)  # Replace with correct transformation\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Output shape test passed!\n",
      "✅ Tensor type test passed!\n",
      "✅ Deterministic output test passed!\n",
      "✅ Masking test passed!\n",
      "🎉 All MultiHeadAttention tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_multi_head_attention():\n",
    "    \"\"\"\n",
    "    Tests the MultiHeadAttention module for correctness.\n",
    "    \"\"\"\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "    d_model = 16\n",
    "    num_heads = 4\n",
    "\n",
    "    # Initialize test input tensors\n",
    "    query = torch.randn(batch_size, seq_len, d_model)\n",
    "    key = torch.randn(batch_size, seq_len, d_model)\n",
    "    value = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # Initialize multi-head attention module\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    # Run forward pass without a mask\n",
    "    output = mha(query, key, value, mask=None)\n",
    "\n",
    "    # Test 1: Check output shape\n",
    "    assert output.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Unexpected output shape: {output.shape}\"\n",
    "    print(\"✅ Output shape test passed!\")\n",
    "\n",
    "    # Test 2: Ensure output is a tensor\n",
    "    assert isinstance(output, torch.Tensor), \"Output is not a tensor\"\n",
    "    print(\"✅ Tensor type test passed!\")\n",
    "\n",
    "    # Test 3: Check deterministic output for same input\n",
    "    output_2 = mha(query, key, value, mask=None)\n",
    "    assert torch.allclose(output, output_2), \"Output should be deterministic!\"\n",
    "    print(\"✅ Deterministic output test passed!\")\n",
    "\n",
    "    # Test 4: Apply a mask and check if masking works\n",
    "    mask = torch.zeros(batch_size, 1, seq_len, seq_len)\n",
    "    mask[:, :, :, -1] = float('-inf')  # Mask the last token\n",
    "\n",
    "    output_masked = mha(query, key, value, mask=mask)\n",
    "\n",
    "    # Ensure output is still the correct shape\n",
    "    assert output_masked.shape == (batch_size, seq_len, d_model), \\\n",
    "        \"Masked output shape mismatch\"\n",
    "    print(\"✅ Masking test passed!\")\n",
    "\n",
    "    print(\"🎉 All MultiHeadAttention tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_multi_head_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model) - Transformed representations.\n",
    "        \"\"\"\n",
    "        output = self.fc1(x)\n",
    "        output = self.relu(output)\n",
    "        output = self.fc2(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Output shape test passed!\n",
      "✅ Tensor type test passed!\n",
      "✅ ReLU activation test passed!\n",
      "✅ Deterministic output test passed!\n",
      "✅ Gradient computation test passed!\n",
      "🎉 All PositionwiseFeedForward tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_positionwise_feedforward():\n",
    "    \"\"\"\n",
    "    Tests the PositionwiseFeedForward module.\n",
    "    \"\"\"\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "    d_model = 16\n",
    "    d_ff = 32  # Expanded dimension\n",
    "\n",
    "    # Initialize test input tensor (random)\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # Initialize the feed-forward module\n",
    "    ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "\n",
    "    # Run forward pass\n",
    "    output = ffn(x)\n",
    "\n",
    "    # Test 1: Check output shape\n",
    "    assert output.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Unexpected output shape: {output.shape}\"\n",
    "    print(\"✅ Output shape test passed!\")\n",
    "\n",
    "    # Test 2: Ensure output is a tensor\n",
    "    assert isinstance(output, torch.Tensor), \"Output is not a tensor\"\n",
    "    print(\"✅ Tensor type test passed!\")\n",
    "\n",
    "    # Test 3: Ensure ReLU activation is applied\n",
    "    hidden_layer_output = ffn.fc1(x)  # Get pre-ReLU values\n",
    "    assert torch.all((hidden_layer_output > 0) == (ffn.relu(hidden_layer_output) > 0)), \\\n",
    "        \"ReLU activation is not applied correctly\"\n",
    "    print(\"✅ ReLU activation test passed!\")\n",
    "\n",
    "    # Test 4: Check deterministic output for same input\n",
    "    output_2 = ffn(x)\n",
    "    assert torch.allclose(output, output_2), \"Output should be deterministic!\"\n",
    "    print(\"✅ Deterministic output test passed!\")\n",
    "\n",
    "    # Test 5: Check gradients (ensuring backpropagation works)\n",
    "    output.sum().backward()  # Compute gradients\n",
    "    assert ffn.fc1.weight.grad is not None, \"Gradients are not computed for fc1!\"\n",
    "    assert ffn.fc2.weight.grad is not None, \"Gradients are not computed for fc2!\"\n",
    "    print(\"✅ Gradient computation test passed!\")\n",
    "\n",
    "    print(\"🎉 All PositionwiseFeedForward tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_positionwise_feedforward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initializes a single Transformer Encoder Layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The embedding dimension (must be divisible by num_heads).\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Hidden layer size of the feed-forward network.\n",
    "            dropout (float): Dropout rate (default 0.1).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Define multi-head self-attention layer\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)  # Replace with MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # TODO: Define feed-forward network (FFN)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff)  # Replace with a two-layer FFN\n",
    "\n",
    "        # TODO: Define Layer Normalization layers\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model) \n",
    "\n",
    "        # TODO: Define Dropout layers\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for Transformer Encoder Layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "            mask (Optional[torch.Tensor]): Mask for attention (default None).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_len, d_model).\n",
    "        \"\"\"\n",
    "        # TODO: Apply multi-head self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask=mask) \n",
    "\n",
    "        # TODO: Apply residual connection and layer normalization\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "\n",
    "        # TODO: Apply feed-forward network\n",
    "        ffn_output = self.ffn(x) \n",
    "\n",
    "        # TODO: Apply second residual connection and layer normalization\n",
    "        x = self.norm2(x + self.dropout2(ffn_output))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEncoderLayer unit test passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def test_transformer_encoder_layer():\n",
    "    # Set a fixed seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Hyperparameters\n",
    "    d_model = 8\n",
    "    num_heads = 2\n",
    "    d_ff = 16\n",
    "    dropout = 0.1\n",
    "    \n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "    \n",
    "    # Instantiate the encoder layer\n",
    "    encoder_layer = TransformerEncoderLayer(\n",
    "        d_model=d_model, \n",
    "        num_heads=num_heads, \n",
    "        d_ff=d_ff, \n",
    "        dropout=dropout\n",
    "    )\n",
    "    \n",
    "    # Create a random input tensor: (batch_size, seq_len, d_model)\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # 1) Forward pass without a mask\n",
    "    out_no_mask = encoder_layer(x)\n",
    "    \n",
    "    # -- Check output shape\n",
    "    assert out_no_mask.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape mismatch. Expected {(batch_size, seq_len, d_model)}, got {out_no_mask.shape}\"\n",
    "    \n",
    "    # 2) Forward pass with a mask\n",
    "    # We'll create a simple mask that zeroes out attention to token #1\n",
    "    # mask shape: (batch_size, 1, seq_len, seq_len)\n",
    "    mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
    "    mask[:, :, :, 1] = 0  # Mask out second token for all queries\n",
    "    \n",
    "    out_with_mask = encoder_layer(x, mask=mask)\n",
    "    \n",
    "    # -- Check output shape\n",
    "    assert out_with_mask.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape mismatch with mask. Expected {(batch_size, seq_len, d_model)}, got {out_with_mask.shape}\"\n",
    "    \n",
    "    # 3) (Optional) Check that the outputs differ when using a mask\n",
    "    #    This may or may not always differ numerically based on random init, but often it does.\n",
    "    #    If you want to ensure they are always different, you can keep this assertion or comment it out.\n",
    "    #    We'll do a \"not allclose\" check with a fairly tight tolerance.\n",
    "    if not torch.allclose(out_no_mask, out_with_mask, atol=1e-5, rtol=1e-5):\n",
    "        pass  # They differ, which is usually good\n",
    "    else:\n",
    "        print(\"Warning: out_no_mask and out_with_mask are numerically very close (maybe random init).\")\n",
    "    \n",
    "    print(\"TransformerEncoderLayer unit test passed!\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "test_transformer_encoder_layer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module): \n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Initializes a single Transformer Decoder Layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The embedding dimension (must be divisible by num_heads).\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Hidden layer size of the feed-forward network.\n",
    "            dropout (float): Dropout rate (default 0.1).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Define masked multi-head self-attention layer\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # TODO: Define multi-head attention layer for encoder-decoder attention\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)  # Replace with MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # TODO: Define feed-forward network (FFN)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "\n",
    "        # TODO: Define Layer Normalization layers\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model) \n",
    "        self.norm3 = nn.LayerNorm(d_model) \n",
    "\n",
    "        # TODO: Define Dropout layers\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, memory: torch.Tensor, \n",
    "                src_mask: Optional[torch.Tensor] = None, \n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for Transformer Decoder Layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model) (decoder input).\n",
    "            memory (torch.Tensor): Encoder outputs of shape (batch_size, seq_len_enc, d_model).\n",
    "            tgt_mask (Optional[torch.Tensor]): Mask for target self-attention (default None).\n",
    "            src_mask (Optional[torch.Tensor]): Mask for encoder-decoder attention (default None).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_len, d_model).\n",
    "        \"\"\"\n",
    "        # TODO: Apply masked multi-head self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask=tgt_mask) \n",
    "\n",
    "        # TODO: Apply residual connection and layer normalization\n",
    "        x = self.norm1(attn_output + self.dropout1(x))\n",
    "\n",
    "        # TODO: Apply encoder-decoder multi-head attention\n",
    "        attn_output_2, _ = self.enc_dec_attn(memory, memory, x, mask=src_mask)\n",
    "\n",
    "        # TODO: Apply residual connection and layer normalization\n",
    "        x = self.norm2(attn_output_2 + self.dropout2(x))\n",
    "\n",
    "        # TODO: Apply feed-forward network\n",
    "        ffn_output = self.ffn(x)\n",
    "\n",
    "        # TODO: Apply final residual connection and layer normalization\n",
    "        x = self.norm3(ffn_output + self.dropout3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Instantiation test passed!\n",
      "✅ Forward pass (no masks) shape test passed!\n",
      "✅ Forward pass (with masks) shape test passed!\n",
      "✅ ReLU activation test passed!\n",
      "✅ Backward pass (gradient) test passed!\n",
      "All TransformerDecoderLayer tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytest\n",
    "\n",
    "def test_transformer_decoder_layer():\n",
    "    \"\"\"\n",
    "    Basic tests for TransformerDecoderLayer to check:\n",
    "    1) Instantiation without errors\n",
    "    2) Forward pass shape consistency\n",
    "    3) Handling of optional masks\n",
    "    4) Presence of ReLU activation (if expected)\n",
    "    5) Gradient backprop flow\n",
    "    \"\"\"\n",
    "    d_model = 32\n",
    "    num_heads = 4\n",
    "    d_ff = 64\n",
    "    dropout = 0.1\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "\n",
    "    # 1) Instantiate the layer\n",
    "    try:\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Instantiation failed with error: {e}\")\n",
    "\n",
    "    print(\"✅ Instantiation test passed!\")\n",
    "\n",
    "    # 2) Create dummy inputs\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    memory = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # 3) Forward pass shape test without masks\n",
    "    try:\n",
    "        output = decoder_layer(x, memory)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed without masks: {e}\")\n",
    "\n",
    "    # Check shape\n",
    "    assert output.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape {output.shape} != {(batch_size, seq_len, d_model)}\"\n",
    "    print(\"✅ Forward pass (no masks) shape test passed!\")\n",
    "\n",
    "    # 4) Forward pass with random masks\n",
    "    src_mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
    "    tgt_mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
    "    try:\n",
    "        output_masked = decoder_layer(x, memory, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed with masks: {e}\")\n",
    "\n",
    "    # Check shape again\n",
    "    assert output_masked.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape with masks {output_masked.shape} != {(batch_size, seq_len, d_model)}\"\n",
    "    print(\"✅ Forward pass (with masks) shape test passed!\")\n",
    "\n",
    "    # 5) (Optional) Check for ReLU activation in the layer\n",
    "    #    Depending on your exact implementation, you may not have a direct ReLU submodule.\n",
    "    found_relu = False\n",
    "    for mod in decoder_layer.modules():\n",
    "        if isinstance(mod, nn.ReLU):\n",
    "            found_relu = True\n",
    "            break\n",
    "    assert found_relu, \"No ReLU found in the TransformerDecoderLayer (if you expected one)!\"\n",
    "    print(\"✅ ReLU activation test passed!\")\n",
    "\n",
    "    # 6) Quick gradient test\n",
    "    #    Make sure we can do a backward pass without errors\n",
    "    output_sum = output_masked.sum()\n",
    "    try:\n",
    "        output_sum.backward()\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Backward pass failed: {e}\")\n",
    "\n",
    "    print(\"✅ Backward pass (gradient) test passed!\")\n",
    "\n",
    "    print(\"All TransformerDecoderLayer tests passed successfully!\")\n",
    "\n",
    "test_transformer_decoder_layer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model) - Input sequence.\n",
    "            mask: (batch_size, 1, seq_len, seq_len) - Optional mask.\n",
    "\n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model) - Encoder output.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, memory: torch.Tensor, \n",
    "                src_mask: Optional[torch.Tensor] = None, \n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model) - Target sequence.\n",
    "            memory: (batch_size, seq_len, d_model) - Encoder output.\n",
    "            src_mask: (batch_size, 1, seq_len, seq_len) - Optional encoder mask.\n",
    "            tgt_mask: (batch_size, 1, seq_len, seq_len) - Optional decoder mask.\n",
    "\n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model) - Decoder output.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, num_layers: int, num_heads: int, \n",
    "                 d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, \n",
    "                src_mask: Optional[torch.Tensor] = None, \n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: (batch_size, src_seq_len) - Source token indices.\n",
    "            tgt: (batch_size, tgt_seq_len) - Target token indices.\n",
    "            src_mask: (batch_size, 1, src_seq_len, src_seq_len) - Optional source mask.\n",
    "            tgt_mask: (batch_size, 1, tgt_seq_len, tgt_seq_len) - Optional target mask.\n",
    "\n",
    "        Returns:\n",
    "            (batch_size, tgt_seq_len, vocab_size) - Token probabilities.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class TransformerTrainer:\n",
    "    def __init__(self, model: Transformer, learning_rate: float, weight_decay: float):\n",
    "        \"\"\"\n",
    "        Initializes optimizer and loss function for training.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def train_step(self, src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Runs a single training step.\n",
    "\n",
    "        Args:\n",
    "            src: (batch_size, src_seq_len) - Source sequence.\n",
    "            tgt: (batch_size, tgt_seq_len) - Target sequence.\n",
    "\n",
    "        Returns:\n",
    "            Loss value.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, src: torch.Tensor, tgt: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Evaluates the model on a validation set.\n",
    "\n",
    "        Args:\n",
    "            src: (batch_size, src_seq_len) - Source sequence.\n",
    "            tgt: (batch_size, tgt_seq_len) - Target sequence.\n",
    "\n",
    "        Returns:\n",
    "            BLEU score or another evaluation metric.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention-is-all-you-need",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
