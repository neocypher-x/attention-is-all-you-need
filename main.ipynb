{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set up an Anaconda environment for implementing the Transformer model in PyTorch, follow these steps:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Create a New Conda Environment**\n",
    "Open a terminal and run:\n",
    "```bash\n",
    "conda create --name attention-is-all-you-need python=3.12\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Activate the Environment**\n",
    "```bash\n",
    "conda activate attention-is-all-you-need\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Install PyTorch**\n",
    "For GPU (CUDA):\n",
    "```bash\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "```\n",
    "For CPU (if you donâ€™t have a compatible GPU):\n",
    "```bash\n",
    "conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
    "```\n",
    "Check if PyTorch is installed correctly:\n",
    "```python\n",
    "python -c \"import torch; print(torch.__version__)\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Install Essential Libraries**\n",
    "```bash\n",
    "pip install numpy pandas matplotlib tqdm\n",
    "```\n",
    "- `numpy`: Tensor operations\n",
    "- `pandas`: Data handling (optional, useful for datasets)\n",
    "- `matplotlib`: Visualization\n",
    "- `tqdm`: Progress bars for training\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Install NLP Libraries (If Needed)**\n",
    "```bash\n",
    "pip install transformers datasets tokenizers sentencepiece\n",
    "```\n",
    "- `transformers`: Pretrained models from Hugging Face (optional)\n",
    "- `datasets`: NLP datasets from Hugging Face\n",
    "- `tokenizers`: Efficient tokenization\n",
    "- `sentencepiece`: Subword tokenization (used in original Transformer)\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Install Jupyter Notebook (Optional)**\n",
    "If you want to develop in Jupyter:\n",
    "```bash\n",
    "conda install jupyter\n",
    "```\n",
    "Then start Jupyter:\n",
    "```bash\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Verify Everything**\n",
    "Run the following to ensure your environment is properly set up:\n",
    "```python\n",
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Save the Environment (Optional)**\n",
    "To export your environment for reproducibility:\n",
    "```bash\n",
    "conda env export > environment.yml\n",
    "```\n",
    "To recreate it later:\n",
    "```bash\n",
    "conda env create -f environment.yml\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int):\n",
    "        \"\"\"\n",
    "        Initializes the embedding layer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Number of unique tokens in the vocabulary.\n",
    "            d_model (int): Dimension of the embedding vectors.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Define the embedding layer that maps token indices to dense vectors.\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)  \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for token embedding.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor of shape (batch_size, seq_len) containing token indices.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of shape (batch_size, seq_len, d_model) containing embedded representations.\n",
    "        \"\"\"\n",
    "        # TODO: Implement the lookup operation using the embedding layer.\n",
    "        embedded = self.embedding(x)  \n",
    "\n",
    "        return embedded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def run_tests():\n",
    "    # Test Parameters\n",
    "    vocab_size = 100\n",
    "    d_model = 16\n",
    "    batch_size = 4\n",
    "    seq_len = 10\n",
    "\n",
    "    # Create a sample input tensor\n",
    "    test_input = torch.randint(0, vocab_size, (batch_size, seq_len))\n",
    "\n",
    "    # Initialize TokenEmbedding\n",
    "    embedding_layer = TokenEmbedding(vocab_size, d_model)\n",
    "\n",
    "    # Test 1: Check Output Shape\n",
    "    output = embedding_layer(test_input)\n",
    "    assert output.shape == (batch_size, seq_len, d_model), f\"Unexpected shape: {output.shape}\"\n",
    "    \n",
    "    # Test 2: Ensure Output is a Tensor of Correct Type\n",
    "    assert isinstance(output, torch.Tensor), \"Output is not a tensor\"\n",
    "    assert output.dtype == torch.float32, f\"Unexpected dtype: {output.dtype}\"\n",
    "    \n",
    "    # Test 3: Check if the Same Token Index Maps to the Same Embedding\n",
    "    index = torch.tensor([[5]])\n",
    "    embedding_1 = embedding_layer(index)\n",
    "    embedding_2 = embedding_layer(index)\n",
    "    assert torch.allclose(embedding_1, embedding_2), \"Embeddings should be identical for the same index\"\n",
    "    \n",
    "    # Test 4: Check if Different Indices Give Different Embeddings\n",
    "    index1 = torch.tensor([[5]])\n",
    "    index2 = torch.tensor([[8]])\n",
    "    embedding_1 = embedding_layer(index1)\n",
    "    embedding_2 = embedding_layer(index2)\n",
    "    assert not torch.allclose(embedding_1, embedding_2), \"Different indices should have different embeddings\"\n",
    "    \n",
    "    # Test 5: Check if Gradients are Computed\n",
    "    loss = output.sum()\n",
    "    loss.backward()\n",
    "    assert embedding_layer.embedding.weight.grad is not None, \"Gradients should not be None\"\n",
    "    assert embedding_layer.embedding.weight.grad.shape == (vocab_size, d_model), \"Gradient shape mismatch\"\n",
    "    \n",
    "    print(\"âœ… All tests passed successfully!\")\n",
    "\n",
    "# Run all tests\n",
    "run_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9214,  0.2546, -1.1575], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = TokenEmbedding(vocab_size=10, d_model=3)\n",
    "embedding_layer(torch.tensor(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import math\n",
    "\n",
    "# class PositionalEncoding(nn.Module):\n",
    "#     def __init__(self, d_model: int, max_len: int = 5000):\n",
    "#         \"\"\"\n",
    "#         Initializes positional encoding.\n",
    "\n",
    "#         Args:\n",
    "#             d_model (int): Dimension of the embedding vectors.\n",
    "#             max_len (int): Maximum sequence length.\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "\n",
    "#         # TODO: Create an empty tensor to hold positional encodings of shape (max_len, d_model)\n",
    "#         pe = torch.zeros(size=(max_len, d_model))\n",
    "\n",
    "#         # TODO: Create a position index tensor of shape (max_len, 1)\n",
    "#         positions = torch.arange(max_len).unsqueeze(1)  # Replace with the correct initialization\n",
    "\n",
    "#         # TODO: Compute the denominator term for the sine/cosine functions\n",
    "#         # div_term = 10**4**(2*positions/d_model)  # Replace with the correct computation\n",
    "#         div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "\n",
    "#         # TODO: Compute sin and cos positional encodings\n",
    "#         # Apply sine to even indices and cosine to odd indices\n",
    "#         # Hint: Use slicing `self.pe[:, 0::2] = ...` for even indices\n",
    "#         #       Use slicing `self.pe[:, 1::2] = ...` for odd indices\n",
    "#         pe[:, 0::2] = torch.sin(positions/div_term)\n",
    "#         pe[:, 1::2] = torch.cos(positions/div_term)\n",
    "\n",
    "#         # TODO: Register `self.pe` as a buffer so it doesn't update during training\n",
    "#         # Hint: Use `self.register_buffer(\"pe\", self.pe)`\n",
    "#         self.register_buffer(\"pe\", pe)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         \"\"\"\n",
    "#         Adds positional encoding to the input embeddings.\n",
    "\n",
    "#         Args:\n",
    "#             x (torch.Tensor): Tensor of shape (batch_size, seq_len, d_model) containing input embeddings.\n",
    "\n",
    "#         Returns:\n",
    "#             torch.Tensor: Tensor of shape (batch_size, seq_len, d_model) with positional encodings added.\n",
    "#         \"\"\"\n",
    "#         # TODO: Retrieve only the necessary positions up to the input sequence length\n",
    "#         # Hint: Slice `self.pe` correctly based on `x.size(1)`\n",
    "#         pe_slice = self.pe[:x.size(1),:].unsqueeze(0)\n",
    "\n",
    "#         # TODO: Add positional encodings to the input embeddings\n",
    "#         # Hint: Ensure the positional encodings are on the same device as `x`\n",
    "#         pe_slice.to(x.device)\n",
    "\n",
    "#         return x + pe_slice  # Replace with the final tensor with positional encoding added\n",
    "    \n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        # Create a (max_len, d_model) tensor to hold the positional encodings\n",
    "        pe = torch.zeros(max_len, d_model)            # shape: (max_len, d_model)\n",
    "        \n",
    "        # position: shape (max_len, 1)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # div_term: shape (d_model/2,)  -> weâ€™ll use it for the even/odd splits\n",
    "        # This follows exp(- log(10000) * (2i/d_model)) = 10000^(-2i/d_model).\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        # Apply sine to even indices (0, 2, 4, ...)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        \n",
    "        # Apply cosine to odd indices (1, 3, 5, ...)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # Register 'pe' as a buffer so it's not trained\n",
    "        self.register_buffer('pe', pe)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: shape (batch_size, seq_len, d_model)\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # Grab up to seq_len positions from pe and add to x\n",
    "        # shape of pe_slice becomes (1, seq_len, d_model)\n",
    "        pe_slice = self.pe[:seq_len, :].unsqueeze(0).to(x.device)\n",
    "\n",
    "        return x + pe_slice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All positional encoding tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "def run_positional_encoding_tests():\n",
    "    d_model = 16\n",
    "    seq_len = 10\n",
    "    batch_size = 4\n",
    "\n",
    "    test_input = torch.zeros((batch_size, seq_len, d_model))  # Placeholder embeddings\n",
    "    pos_encoding = PositionalEncoding(d_model=d_model)\n",
    "\n",
    "    # Test 1: Check Output Shape\n",
    "    output = pos_encoding(test_input)\n",
    "    assert output.shape == (batch_size, seq_len, d_model), f\"Unexpected shape: {output.shape}\"\n",
    "    \n",
    "    # Test 2: Ensure Output is a Tensor of Correct Type\n",
    "    assert isinstance(output, torch.Tensor), \"Output is not a tensor\"\n",
    "    assert output.dtype == torch.float32, f\"Unexpected dtype: {output.dtype}\"\n",
    "    \n",
    "    # Test 3: Check if Positional Encoding is Being Added\n",
    "    assert not torch.allclose(test_input, output), \"Positional encoding is not being added!\"\n",
    "    \n",
    "    # Test 4: Check Device Compatibility\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    test_input = test_input.to(device)\n",
    "    pos_encoding = pos_encoding.to(device)\n",
    "    output = pos_encoding(test_input)\n",
    "    assert output.device == test_input.device, f\"Device mismatch: {output.device} vs {test_input.device}\"\n",
    "    \n",
    "    # Test 5: Check if Encodings are Deterministic\n",
    "    output1 = pos_encoding(test_input)\n",
    "    output2 = pos_encoding(test_input)\n",
    "    assert torch.allclose(output1, output2), \"Positional encoding should be deterministic!\"\n",
    "    \n",
    "    print(\"âœ… All positional encoding tests passed successfully!\")\n",
    "\n",
    "# Run all tests\n",
    "run_positional_encoding_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+01],\n",
      "        [2.0869e+01],\n",
      "        [5.5094e+01],\n",
      "        [1.9833e+02],\n",
      "        [1.0751e+03],\n",
      "        [1.0000e+04],\n",
      "        [1.8968e+05],\n",
      "        [9.2131e+06],\n",
      "        [1.5473e+09],\n",
      "        [1.3358e+12],\n",
      "        [1.0000e+16],\n",
      "        [1.2946e+21],\n",
      "        [7.2048e+27]])\n"
     ]
    }
   ],
   "source": [
    "def scratchboard(max_len, d_model):\n",
    "    pe = torch.zeros(size=(max_len, d_model))\n",
    "    positions = torch.arange(max_len).unsqueeze(1)\n",
    "    div_term = 10**4**(2*positions/d_model)\n",
    "    print(div_term)\n",
    "\n",
    "scratchboard(13, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from typing import Optional, Tuple\n",
    "\n",
    "# class ScaledDotProductAttention(nn.Module):\n",
    "#     def __init__(self, d_k: int):\n",
    "#         \"\"\"\n",
    "#         Initializes scaled dot-product attention.\n",
    "\n",
    "#         Args:\n",
    "#             d_k (int): Dimension of the key vectors (used for scaling).\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "\n",
    "#         # TODO: Store d_k for scaling attention scores\n",
    "#         self.d_k = d_k  # Replace with correct initialization\n",
    "\n",
    "#     def forward(\n",
    "#         self, \n",
    "#         query: torch.Tensor, \n",
    "#         key: torch.Tensor, \n",
    "#         value: torch.Tensor, \n",
    "#         mask: Optional[torch.Tensor] = None\n",
    "#     ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "#         \"\"\"\n",
    "#         Computes the scaled dot-product attention.\n",
    "\n",
    "#         Args:\n",
    "#             query (torch.Tensor): Shape (batch_size, num_heads, seq_len, d_k)\n",
    "#             key (torch.Tensor): Shape (batch_size, num_heads, seq_len, d_k)\n",
    "#             value (torch.Tensor): Shape (batch_size, num_heads, seq_len, d_v)\n",
    "#             mask (Optional[torch.Tensor]): Shape (batch_size, 1, seq_len, seq_len) \n",
    "#                                            (mask for padding or future tokens in decoder)\n",
    "\n",
    "#         Returns:\n",
    "#             Tuple[torch.Tensor, torch.Tensor]: \n",
    "#                 - Attention output of shape (batch_size, num_heads, seq_len, d_v)\n",
    "#                 - Attention weights of shape (batch_size, num_heads, seq_len, seq_len)\n",
    "#         \"\"\"\n",
    "#         # TODO: Compute attention scores as QK^T / sqrt(d_k)\n",
    "#         attention_scores = torch.matmul(query, torch.transpose(key, -2, -1)) / math.sqrt(self.d_k)   # Replace with correct computation\n",
    "\n",
    "#         # TODO: Apply mask (if provided) by setting masked positions to a very low value\n",
    "#         # Hint: Use `float('-inf')` for masked positions before applying softmax\n",
    "#         if mask is not None:\n",
    "#             attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "#         # TODO: Compute attention weights using softmax\n",
    "#         attention_weights = torch.softmax(attention_scores, dim=-1)  # Replace with correct computation\n",
    "\n",
    "#         # TODO: Multiply attention weights by value matrix to get the final output\n",
    "#         output = torch.matmul(attention_weights, value)  # Replace with correct computation\n",
    "\n",
    "#         return output, attention_weights  # Return attention output and weights\n",
    "    \n",
    "\n",
    "import math\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_k: int):\n",
    "        super().__init__()\n",
    "        self.d_k = d_k   # for scaling\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,   # (batch_size, num_heads, seq_len, d_k)\n",
    "        key: torch.Tensor,     # (batch_size, num_heads, seq_len, d_k)\n",
    "        value: torch.Tensor,   # (batch_size, num_heads, seq_len, d_v)\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        # (1) QK^T\n",
    "        # key.transpose(-2, -1) is shape (batch_size, num_heads, d_k, seq_len)\n",
    "        attention_scores = torch.matmul(query, key.transpose(-2, -1))\n",
    "        \n",
    "        # (2) Scale by sqrt(d_k)\n",
    "        attention_scores = attention_scores / math.sqrt(self.d_k)\n",
    "\n",
    "        # (3) If mask is provided, set masked positions to -inf\n",
    "        if mask is not None:\n",
    "            # Typically a 1/0 mask is used; we want to fill 0â€™s with -inf\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # (4) Apply softmax over the last dimension (seq_len of the key)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # (5) Multiply by V\n",
    "        output = torch.matmul(attention_weights, value)\n",
    "\n",
    "        return output, attention_weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledDotProductAttention unit test passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "def test_scaled_dot_product_attention():\n",
    "    # Make some deterministic random data.\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    batch_size = 2\n",
    "    num_heads = 3\n",
    "    seq_len_q = 4  # length of the query\n",
    "    seq_len_k = 5  # length of the key\n",
    "    d_k = 6        # dimension per head for query/key\n",
    "    d_v = 6        # dimension per head for value\n",
    "\n",
    "    # Create a random ScaledDotProductAttention instance\n",
    "    attention_module = ScaledDotProductAttention(d_k)\n",
    "\n",
    "    # Create random query, key, value\n",
    "    query = torch.randn(batch_size, num_heads, seq_len_q, d_k)\n",
    "    key   = torch.randn(batch_size, num_heads, seq_len_k, d_k)\n",
    "    value = torch.randn(batch_size, num_heads, seq_len_k, d_v)\n",
    "\n",
    "    # (1) Test forward pass without mask\n",
    "    output, attn_weights = attention_module(query, key, value, mask=None)\n",
    "    \n",
    "    #  -- Check output shape = (batch_size, num_heads, seq_len_q, d_v)\n",
    "    assert output.shape == (batch_size, num_heads, seq_len_q, d_v), \\\n",
    "        f\"Output shape mismatch. Got {output.shape}\"\n",
    "    \n",
    "    #  -- Check attention weight shape = (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    assert attn_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k), \\\n",
    "        f\"Attention weights shape mismatch. Got {attn_weights.shape}\"\n",
    "    \n",
    "    #  -- Check attention weights sum to ~1 across last dimension\n",
    "    attn_sum = attn_weights.sum(dim=-1)\n",
    "    assert torch.allclose(attn_sum, torch.ones_like(attn_sum), atol=1e-5), \\\n",
    "        \"Attention weights do not sum to 1 along the last dimension.\"\n",
    "    \n",
    "    # (2) Test forward pass with a mask (e.g., masking out the last two positions)\n",
    "    #     We'll create a mask of shape (batch_size, 1, seq_len_q, seq_len_k).\n",
    "    #     Suppose we only want the first 3 positions of the key unmasked:\n",
    "    mask = torch.ones(batch_size, 1, seq_len_q, seq_len_k)\n",
    "    mask[:, :, :, -2:] = 0  # mask out the last 2 positions\n",
    "    output_masked, attn_weights_masked = attention_module(query, key, value, mask=mask)\n",
    "\n",
    "    #  -- The masked positions in the softmax should drop to near 0\n",
    "    #     Weâ€™ll check the last two positions of each attention row in attn_weights_masked\n",
    "    #     are effectively 0 (within a floating tolerance).\n",
    "    masked_positions = attn_weights_masked[..., -2:]  # shape (batch_size, num_heads, seq_len_q, 2)\n",
    "    assert torch.allclose(masked_positions, torch.zeros_like(masked_positions), atol=1e-5), \\\n",
    "        \"Masking does not appear to zero out the last two positions.\"\n",
    "\n",
    "    print(\"ScaledDotProductAttention unit test passed!\")\n",
    "\n",
    "\n",
    "# Example usage in a Jupyter cell:\n",
    "test_scaled_dot_product_attention()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        \"\"\"\n",
    "        Initializes multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Dimension of the model (input and output size).\n",
    "            num_heads (int): Number of attention heads.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Ensure d_model is divisible by num_heads\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # Dimension per head\n",
    "\n",
    "        # TODO: Define linear transformations for query, key, and value\n",
    "        self.W_q = nn.Linear(d_model, d_model)  # Replace with nn.Linear\n",
    "        self.W_k = nn.Linear(d_model, d_model)  # Replace with nn.Linear\n",
    "        self.W_v = nn.Linear(d_model, d_model)  # Replace with nn.Linear\n",
    "\n",
    "        # TODO: Define output projection layer\n",
    "        self.W_o = nn.Linear(d_model, d_model)  # Replace with nn.Linear\n",
    "\n",
    "        # TODO: Define the scaled dot-product attention module\n",
    "        self.attention = ScaledDotProductAttention(self.d_k)  # Replace with ScaledDotProductAttention(self.d_k)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        value: torch.Tensor,\n",
    "        mask: Optional[torch.Tensor] = None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): Shape (batch_size, seq_len, d_model)\n",
    "            key (torch.Tensor): Shape (batch_size, seq_len, d_model)\n",
    "            value (torch.Tensor): Shape (batch_size, seq_len, d_model)\n",
    "            mask (Optional[torch.Tensor]): Shape (batch_size, 1, seq_len, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Shape (batch_size, seq_len, d_model) - Multi-head attention output.\n",
    "        \"\"\"\n",
    "        # TODO: Apply linear transformations to query, key, and value\n",
    "        Q = self.W_q(query)  # Replace with correct transformation\n",
    "        K = self.W_k(key)  # Replace with correct transformation\n",
    "        V = self.W_v(value)  # Replace with correct transformation\n",
    "\n",
    "        # TODO: Reshape Q, K, V for multi-head attention\n",
    "        # Hint: Use `.view()` and `.transpose()` to shape into (batch_size, num_heads, seq_len, d_k)\n",
    "        batch_size, seq_len, _ = query.shape\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "\n",
    "        Q = Q.transpose(1, 2)\n",
    "        K = K.transpose(1, 2)\n",
    "        V = V.transpose(1, 2)\n",
    "\n",
    "        # TODO: Apply scaled dot-product attention\n",
    "        output, attention_weights = self.attention(Q, K, V, mask)  # Replace with correct computation\n",
    "\n",
    "        # TODO: Concatenate the heads back and apply final linear transformation\n",
    "        # Current shape: (batch_size, num_heads, seq_len, d_k)\n",
    "        # We first swap num_heads and seq_len\n",
    "        output = output.transpose(1, 2)  # (batch_size, seq_len, num_heads, d_k)\n",
    "        output = output.contiguous().view(batch_size, seq_len, self.d_model)\n",
    "\n",
    "        output = self.W_o(output)  # Replace with correct transformation\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Output shape test passed!\n",
      "âœ… Tensor type test passed!\n",
      "âœ… Deterministic output test passed!\n",
      "âœ… Masking test passed!\n",
      "ðŸŽ‰ All MultiHeadAttention tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_multi_head_attention():\n",
    "    \"\"\"\n",
    "    Tests the MultiHeadAttention module for correctness.\n",
    "    \"\"\"\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "    d_model = 16\n",
    "    num_heads = 4\n",
    "\n",
    "    # Initialize test input tensors\n",
    "    query = torch.randn(batch_size, seq_len, d_model)\n",
    "    key = torch.randn(batch_size, seq_len, d_model)\n",
    "    value = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # Initialize multi-head attention module\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    # Run forward pass without a mask\n",
    "    output = mha(query, key, value, mask=None)\n",
    "\n",
    "    # Test 1: Check output shape\n",
    "    assert output.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Unexpected output shape: {output.shape}\"\n",
    "    print(\"âœ… Output shape test passed!\")\n",
    "\n",
    "    # Test 2: Ensure output is a tensor\n",
    "    assert isinstance(output, torch.Tensor), \"Output is not a tensor\"\n",
    "    print(\"âœ… Tensor type test passed!\")\n",
    "\n",
    "    # Test 3: Check deterministic output for same input\n",
    "    output_2 = mha(query, key, value, mask=None)\n",
    "    assert torch.allclose(output, output_2), \"Output should be deterministic!\"\n",
    "    print(\"âœ… Deterministic output test passed!\")\n",
    "\n",
    "    # Test 4: Apply a mask and check if masking works\n",
    "    mask = torch.zeros(batch_size, 1, seq_len, seq_len)\n",
    "    mask[:, :, :, -1] = float('-inf')  # Mask the last token\n",
    "\n",
    "    output_masked = mha(query, key, value, mask=mask)\n",
    "\n",
    "    # Ensure output is still the correct shape\n",
    "    assert output_masked.shape == (batch_size, seq_len, d_model), \\\n",
    "        \"Masked output shape mismatch\"\n",
    "    print(\"âœ… Masking test passed!\")\n",
    "\n",
    "    print(\"ðŸŽ‰ All MultiHeadAttention tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_multi_head_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model: int, d_ff: int):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model) - Transformed representations.\n",
    "        \"\"\"\n",
    "        output = self.fc1(x)\n",
    "        output = self.relu(output)\n",
    "        output = self.fc2(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Output shape test passed!\n",
      "âœ… Tensor type test passed!\n",
      "âœ… ReLU activation test passed!\n",
      "âœ… Deterministic output test passed!\n",
      "âœ… Gradient computation test passed!\n",
      "ðŸŽ‰ All PositionwiseFeedForward tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_positionwise_feedforward():\n",
    "    \"\"\"\n",
    "    Tests the PositionwiseFeedForward module.\n",
    "    \"\"\"\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "    d_model = 16\n",
    "    d_ff = 32  # Expanded dimension\n",
    "\n",
    "    # Initialize test input tensor (random)\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # Initialize the feed-forward module\n",
    "    ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "\n",
    "    # Run forward pass\n",
    "    output = ffn(x)\n",
    "\n",
    "    # Test 1: Check output shape\n",
    "    assert output.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Unexpected output shape: {output.shape}\"\n",
    "    print(\"âœ… Output shape test passed!\")\n",
    "\n",
    "    # Test 2: Ensure output is a tensor\n",
    "    assert isinstance(output, torch.Tensor), \"Output is not a tensor\"\n",
    "    print(\"âœ… Tensor type test passed!\")\n",
    "\n",
    "    # Test 3: Ensure ReLU activation is applied\n",
    "    hidden_layer_output = ffn.fc1(x)  # Get pre-ReLU values\n",
    "    assert torch.all((hidden_layer_output > 0) == (ffn.relu(hidden_layer_output) > 0)), \\\n",
    "        \"ReLU activation is not applied correctly\"\n",
    "    print(\"âœ… ReLU activation test passed!\")\n",
    "\n",
    "    # Test 4: Check deterministic output for same input\n",
    "    output_2 = ffn(x)\n",
    "    assert torch.allclose(output, output_2), \"Output should be deterministic!\"\n",
    "    print(\"âœ… Deterministic output test passed!\")\n",
    "\n",
    "    # Test 5: Check gradients (ensuring backpropagation works)\n",
    "    output.sum().backward()  # Compute gradients\n",
    "    assert ffn.fc1.weight.grad is not None, \"Gradients are not computed for fc1!\"\n",
    "    assert ffn.fc2.weight.grad is not None, \"Gradients are not computed for fc2!\"\n",
    "    print(\"âœ… Gradient computation test passed!\")\n",
    "\n",
    "    print(\"ðŸŽ‰ All PositionwiseFeedForward tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_positionwise_feedforward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        \"\"\"\n",
    "        Initializes a single Transformer Encoder Layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The embedding dimension (must be divisible by num_heads).\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Hidden layer size of the feed-forward network.\n",
    "            dropout (float): Dropout rate (default 0.1).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Define multi-head self-attention layer\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)  # Replace with MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # TODO: Define feed-forward network (FFN)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff)  # Replace with a two-layer FFN\n",
    "\n",
    "        # TODO: Define Layer Normalization layers\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model) \n",
    "\n",
    "        # TODO: Define Dropout layers\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for Transformer Encoder Layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model).\n",
    "            mask (Optional[torch.Tensor]): Mask for attention (default None).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_len, d_model).\n",
    "        \"\"\"\n",
    "        # TODO: Apply multi-head self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask=mask) \n",
    "\n",
    "        # TODO: Apply residual connection and layer normalization\n",
    "        x = self.norm1(x + self.dropout1(attn_output))\n",
    "\n",
    "        # TODO: Apply feed-forward network\n",
    "        ffn_output = self.ffn(x) \n",
    "\n",
    "        # TODO: Apply second residual connection and layer normalization\n",
    "        x = self.norm2(x + self.dropout2(ffn_output))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEncoderLayer unit test passed!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def test_transformer_encoder_layer():\n",
    "    # Set a fixed seed for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Hyperparameters\n",
    "    d_model = 8\n",
    "    num_heads = 2\n",
    "    d_ff = 16\n",
    "    dropout = 0.1\n",
    "    \n",
    "    batch_size = 2\n",
    "    seq_len = 4\n",
    "    \n",
    "    # Instantiate the encoder layer\n",
    "    encoder_layer = TransformerEncoderLayer(\n",
    "        d_model=d_model, \n",
    "        num_heads=num_heads, \n",
    "        d_ff=d_ff, \n",
    "        dropout=dropout\n",
    "    )\n",
    "    \n",
    "    # Create a random input tensor: (batch_size, seq_len, d_model)\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # 1) Forward pass without a mask\n",
    "    out_no_mask = encoder_layer(x)\n",
    "    \n",
    "    # -- Check output shape\n",
    "    assert out_no_mask.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape mismatch. Expected {(batch_size, seq_len, d_model)}, got {out_no_mask.shape}\"\n",
    "    \n",
    "    # 2) Forward pass with a mask\n",
    "    # We'll create a simple mask that zeroes out attention to token #1\n",
    "    # mask shape: (batch_size, 1, seq_len, seq_len)\n",
    "    mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
    "    mask[:, :, :, 1] = 0  # Mask out second token for all queries\n",
    "    \n",
    "    out_with_mask = encoder_layer(x, mask=mask)\n",
    "    \n",
    "    # -- Check output shape\n",
    "    assert out_with_mask.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape mismatch with mask. Expected {(batch_size, seq_len, d_model)}, got {out_with_mask.shape}\"\n",
    "    \n",
    "    # 3) (Optional) Check that the outputs differ when using a mask\n",
    "    #    This may or may not always differ numerically based on random init, but often it does.\n",
    "    #    If you want to ensure they are always different, you can keep this assertion or comment it out.\n",
    "    #    We'll do a \"not allclose\" check with a fairly tight tolerance.\n",
    "    if not torch.allclose(out_no_mask, out_with_mask, atol=1e-5, rtol=1e-5):\n",
    "        pass  # They differ, which is usually good\n",
    "    else:\n",
    "        print(\"Warning: out_no_mask and out_with_mask are numerically very close (maybe random init).\")\n",
    "    \n",
    "    print(\"TransformerEncoderLayer unit test passed!\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "test_transformer_encoder_layer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module): \n",
    "    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float):\n",
    "        \"\"\"\n",
    "        Initializes a single Transformer Decoder Layer.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The embedding dimension (must be divisible by num_heads).\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Hidden layer size of the feed-forward network.\n",
    "            dropout (float): Dropout rate (default 0.1).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # TODO: Define masked multi-head self-attention layer\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # TODO: Define multi-head attention layer for encoder-decoder attention\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)  # Replace with MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        # TODO: Define feed-forward network (FFN)\n",
    "        self.ffn = PositionwiseFeedForward(d_model, d_ff)\n",
    "\n",
    "        # TODO: Define Layer Normalization layers\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model) \n",
    "        self.norm3 = nn.LayerNorm(d_model) \n",
    "\n",
    "        # TODO: Define Dropout layers\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, memory: torch.Tensor, \n",
    "                src_mask: Optional[torch.Tensor] = None, \n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for Transformer Decoder Layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, d_model) (decoder input).\n",
    "            memory (torch.Tensor): Encoder outputs of shape (batch_size, seq_len_enc, d_model).\n",
    "            tgt_mask (Optional[torch.Tensor]): Mask for target self-attention (default None).\n",
    "            src_mask (Optional[torch.Tensor]): Mask for encoder-decoder attention (default None).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch_size, seq_len, d_model).\n",
    "        \"\"\"\n",
    "        # TODO: Apply masked multi-head self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask=tgt_mask) \n",
    "\n",
    "        # TODO: Apply residual connection and layer normalization\n",
    "        x = self.norm1(attn_output + self.dropout1(x))\n",
    "\n",
    "        # TODO: Apply encoder-decoder multi-head attention\n",
    "        attn_output_2, _ = self.enc_dec_attn(memory, memory, x, mask=src_mask)\n",
    "\n",
    "        # TODO: Apply residual connection and layer normalization\n",
    "        x = self.norm2(attn_output_2 + self.dropout2(x))\n",
    "\n",
    "        # TODO: Apply feed-forward network\n",
    "        ffn_output = self.ffn(x)\n",
    "\n",
    "        # TODO: Apply final residual connection and layer normalization\n",
    "        x = self.norm3(ffn_output + self.dropout3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Instantiation test passed!\n",
      "âœ… Forward pass (no masks) shape test passed!\n",
      "âœ… Forward pass (with masks) shape test passed!\n",
      "âœ… ReLU activation test passed!\n",
      "âœ… Backward pass (gradient) test passed!\n",
      "All TransformerDecoderLayer tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytest\n",
    "\n",
    "def test_transformer_decoder_layer():\n",
    "    \"\"\"\n",
    "    Basic tests for TransformerDecoderLayer to check:\n",
    "    1) Instantiation without errors\n",
    "    2) Forward pass shape consistency\n",
    "    3) Handling of optional masks\n",
    "    4) Presence of ReLU activation (if expected)\n",
    "    5) Gradient backprop flow\n",
    "    \"\"\"\n",
    "    d_model = 32\n",
    "    num_heads = 4\n",
    "    d_ff = 64\n",
    "    dropout = 0.1\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "\n",
    "    # 1) Instantiate the layer\n",
    "    try:\n",
    "        decoder_layer = TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Instantiation failed with error: {e}\")\n",
    "\n",
    "    print(\"âœ… Instantiation test passed!\")\n",
    "\n",
    "    # 2) Create dummy inputs\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    memory = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "    # 3) Forward pass shape test without masks\n",
    "    try:\n",
    "        output = decoder_layer(x, memory)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed without masks: {e}\")\n",
    "\n",
    "    # Check shape\n",
    "    assert output.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape {output.shape} != {(batch_size, seq_len, d_model)}\"\n",
    "    print(\"âœ… Forward pass (no masks) shape test passed!\")\n",
    "\n",
    "    # 4) Forward pass with random masks\n",
    "    src_mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
    "    tgt_mask = torch.ones(batch_size, 1, seq_len, seq_len)\n",
    "    try:\n",
    "        output_masked = decoder_layer(x, memory, src_mask=src_mask, tgt_mask=tgt_mask)\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Forward pass failed with masks: {e}\")\n",
    "\n",
    "    # Check shape again\n",
    "    assert output_masked.shape == (batch_size, seq_len, d_model), \\\n",
    "        f\"Output shape with masks {output_masked.shape} != {(batch_size, seq_len, d_model)}\"\n",
    "    print(\"âœ… Forward pass (with masks) shape test passed!\")\n",
    "\n",
    "    # 5) (Optional) Check for ReLU activation in the layer\n",
    "    #    Depending on your exact implementation, you may not have a direct ReLU submodule.\n",
    "    found_relu = False\n",
    "    for mod in decoder_layer.modules():\n",
    "        if isinstance(mod, nn.ReLU):\n",
    "            found_relu = True\n",
    "            break\n",
    "    assert found_relu, \"No ReLU found in the TransformerDecoderLayer (if you expected one)!\"\n",
    "    print(\"âœ… ReLU activation test passed!\")\n",
    "\n",
    "    # 6) Quick gradient test\n",
    "    #    Make sure we can do a backward pass without errors\n",
    "    output_sum = output_masked.sum()\n",
    "    try:\n",
    "        output_sum.backward()\n",
    "    except Exception as e:\n",
    "        pytest.fail(f\"Backward pass failed: {e}\")\n",
    "\n",
    "    print(\"âœ… Backward pass (gradient) test passed!\")\n",
    "\n",
    "    print(\"All TransformerDecoderLayer tests passed successfully!\")\n",
    "\n",
    "test_transformer_decoder_layer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model) - Input sequence.\n",
    "            mask: (batch_size, 1, seq_len, seq_len) - Optional mask.\n",
    "\n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model) - Encoder output.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_layers: int, d_model: int, num_heads: int, d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x: torch.Tensor, memory: torch.Tensor, \n",
    "                src_mask: Optional[torch.Tensor] = None, \n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: (batch_size, seq_len, d_model) - Target sequence.\n",
    "            memory: (batch_size, seq_len, d_model) - Encoder output.\n",
    "            src_mask: (batch_size, 1, seq_len, seq_len) - Optional encoder mask.\n",
    "            tgt_mask: (batch_size, 1, seq_len, seq_len) - Optional decoder mask.\n",
    "\n",
    "        Returns:\n",
    "            (batch_size, seq_len, d_model) - Decoder output.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int, num_layers: int, num_heads: int, \n",
    "                 d_ff: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor, \n",
    "                src_mask: Optional[torch.Tensor] = None, \n",
    "                tgt_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: (batch_size, src_seq_len) - Source token indices.\n",
    "            tgt: (batch_size, tgt_seq_len) - Target token indices.\n",
    "            src_mask: (batch_size, 1, src_seq_len, src_seq_len) - Optional source mask.\n",
    "            tgt_mask: (batch_size, 1, tgt_seq_len, tgt_seq_len) - Optional target mask.\n",
    "\n",
    "        Returns:\n",
    "            (batch_size, tgt_seq_len, vocab_size) - Token probabilities.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "class TransformerTrainer:\n",
    "    def __init__(self, model: Transformer, learning_rate: float, weight_decay: float):\n",
    "        \"\"\"\n",
    "        Initializes optimizer and loss function for training.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def train_step(self, src: torch.Tensor, tgt: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Runs a single training step.\n",
    "\n",
    "        Args:\n",
    "            src: (batch_size, src_seq_len) - Source sequence.\n",
    "            tgt: (batch_size, tgt_seq_len) - Target sequence.\n",
    "\n",
    "        Returns:\n",
    "            Loss value.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def evaluate(self, src: torch.Tensor, tgt: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Evaluates the model on a validation set.\n",
    "\n",
    "        Args:\n",
    "            src: (batch_size, src_seq_len) - Source sequence.\n",
    "            tgt: (batch_size, tgt_seq_len) - Target sequence.\n",
    "\n",
    "        Returns:\n",
    "            BLEU score or another evaluation metric.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention-is-all-you-need",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
