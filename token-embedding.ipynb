{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: False\n",
      "âœ… All tests passed successfully!\n",
      "âœ… All positional encoding tests passed successfully!\n",
      "tensor([[1.0000e+01],\n",
      "        [2.0869e+01],\n",
      "        [5.5094e+01],\n",
      "        [1.9833e+02],\n",
      "        [1.0751e+03],\n",
      "        [1.0000e+04],\n",
      "        [1.8968e+05],\n",
      "        [9.2131e+06],\n",
      "        [1.5473e+09],\n",
      "        [1.3358e+12],\n",
      "        [1.0000e+16],\n",
      "        [1.2946e+21],\n",
      "        [7.2048e+27]])\n",
      "âœ… Output shape test passed!\n",
      "âœ… Attention weights shape test passed!\n",
      "âœ… Softmax test passed!\n",
      "âœ… Deterministic output test passed!\n",
      "âœ… Masking test passed!\n",
      "ðŸŽ‰ All ScaledDotProductAttention tests passed!\n",
      "âœ… Output shape test passed!\n",
      "âœ… Tensor type test passed!\n",
      "âœ… Deterministic output test passed!\n",
      "âœ… Masking test passed!\n",
      "ðŸŽ‰ All MultiHeadAttention tests passed!\n",
      "âœ… Output shape test passed!\n",
      "âœ… Tensor type test passed!\n",
      "âœ… ReLU activation test passed!\n",
      "âœ… Deterministic output test passed!\n",
      "âœ… Gradient computation test passed!\n",
      "ðŸŽ‰ All PositionwiseFeedForward tests passed!\n"
     ]
    }
   ],
   "source": [
    "# executes all the code in .ipynb, making its variables, functions, and classes available in the current notebook\n",
    "%run main.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TokenEmbedding` class is essentially a simple wrapper around PyTorch's `nn.Embedding` layer. Let's break down its role in the Transformer model and develop some intuition for why it's needed.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Do We Need Token Embeddings?**\n",
    "Transformers operate on continuous-valued vectors, but raw text is represented as discrete token indices. The `TokenEmbedding` layer maps each token index (a unique integer in a vocabulary) to a learnable dense vector of size `d_model`. These embeddings serve as input features to the Transformer.\n",
    "\n",
    "---\n",
    "\n",
    "### **Understanding the Components**\n",
    "1. **`nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)`**\n",
    "   - `num_embeddings=vocab_size`: This defines the number of unique tokens in the vocabulary.\n",
    "   - `embedding_dim=d_model`: Each token is mapped to a `d_model`-dimensional dense vector.\n",
    "\n",
    "2. **`forward(x)`**\n",
    "   - The input `x` is a tensor of shape `(batch_size, seq_len)`, where each value in `x` is an integer representing a token index.\n",
    "   - The `nn.Embedding` layer takes these indices and outputs a tensor of shape `(batch_size, seq_len, d_model)`, where each token index is replaced with its corresponding learned embedding vector.\n",
    "\n",
    "---\n",
    "\n",
    "### **Intuition**\n",
    "Think of an embedding layer as a lookup table:\n",
    "- Suppose `vocab_size = 10,000` and `d_model = 512`.\n",
    "- The embedding layer maintains a weight matrix of shape `(10,000, 512)`, where each row represents the learnable embedding vector for a token.\n",
    "- When we input a sequence of token indices (e.g., `[2, 45, 398]`), the layer extracts the corresponding rows from the embedding matrix and returns them as a tensor.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Usage**\n",
    "```python\n",
    "vocab_size = 10000  # Example vocab size\n",
    "d_model = 512  # Embedding dimension\n",
    "\n",
    "embedding_layer = TokenEmbedding(vocab_size, d_model)\n",
    "\n",
    "# Example input: batch of 2 sequences of length 4\n",
    "x = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])\n",
    "\n",
    "embedded_x = embedding_layer(x)\n",
    "print(embedded_x.shape)  # Expected output: (2, 4, 512)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Not One-Hot Encoding?**\n",
    "Instead of learning embeddings, we could represent tokens using one-hot encoding. However, one-hot encoding has limitations:\n",
    "- The vector size would be `vocab_size`, making it infeasible for large vocabularies.\n",
    "- One-hot vectors don't capture semantic relationships between words (e.g., \"king\" and \"queen\" would be completely different vectors).\n",
    "\n",
    "By learning embeddings, the model can discover useful relationships between words in a lower-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thatâ€™s a great idea! Playing around with `TokenEmbedding` in a Jupyter Notebook will give you a strong intuition about how embeddings transform discrete tokens into dense vector representations. Hereâ€™s a step-by-step approach to explore it interactively:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Setup**\n",
    "First, ensure that PyTorch is installed:\n",
    "```python\n",
    "!pip install torch\n",
    "```\n",
    "\n",
    "Then, import necessary libraries:\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Define `TokenEmbedding`**\n",
    "Weâ€™ll define the `TokenEmbedding` class as you originally wrote it:\n",
    "```python\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, d_model: int):\n",
    "        \"\"\"\n",
    "        Initializes the embedding layer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Number of unique tokens in the vocabulary.\n",
    "            d_model (int): Dimension of the embedding vectors.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)  \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for token embedding.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor of shape (batch_size, seq_len) containing token indices.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Tensor of shape (batch_size, seq_len, d_model) containing embedded representations.\n",
    "        \"\"\"\n",
    "        return self.embedding(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Initialize the Embedding Layer**\n",
    "Set up an example vocabulary size and embedding dimension:\n",
    "```python\n",
    "vocab_size = 20  # Suppose our vocab has 20 words\n",
    "d_model = 5      # Each token gets a 5-dimensional embedding\n",
    "\n",
    "embedding_layer = TokenEmbedding(vocab_size, d_model)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Pass Sample Input**\n",
    "Create a batch of tokenized sequences and feed them into the embedding layer:\n",
    "```python\n",
    "# Example tokenized sequences (batch_size=2, seq_len=4)\n",
    "x = torch.tensor([[1, 5, 10, 15], [2, 6, 11, 16]])\n",
    "\n",
    "# Pass through the embedding layer\n",
    "embedded_x = embedding_layer(x)\n",
    "\n",
    "# Print shapes\n",
    "print(f\"Input shape: {x.shape}\")  # (batch_size, seq_len)\n",
    "print(f\"Output shape: {embedded_x.shape}\")  # (batch_size, seq_len, d_model)\n",
    "\n",
    "# View actual embeddings\n",
    "print(\"\\nEmbedded representations:\\n\", embedded_x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Visualizing the Embeddings**\n",
    "### **Checking the Embedding Matrix**\n",
    "Since `nn.Embedding` maintains a lookup table of size `(vocab_size, d_model)`, we can directly inspect it:\n",
    "```python\n",
    "print(\"Embedding matrix (weight parameters):\\n\")\n",
    "print(embedding_layer.embedding.weight)  # Shows the entire embedding matrix\n",
    "```\n",
    "\n",
    "Each row corresponds to a word in the vocabulary, and each column represents one of its embedding dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Experimenting with Different Inputs**\n",
    "Try passing:\n",
    "- A single sequence instead of a batch.\n",
    "- A single token index.\n",
    "- A random tensor with invalid values (to see the error).\n",
    "```python\n",
    "# Single sequence\n",
    "single_seq = torch.tensor([3, 7, 14])\n",
    "print(\"\\nSingle sequence embedding:\\n\", embedding_layer(single_seq))\n",
    "\n",
    "# Single token index\n",
    "single_token = torch.tensor([4])\n",
    "print(\"\\nSingle token embedding:\\n\", embedding_layer(single_token))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **What's Next?**\n",
    "- Try increasing `d_model` to see how it affects embeddings.\n",
    "- Use different token values to verify the lookup behavior.\n",
    "- Add `PositionalEncoding` to see how positional information is incorporated.\n",
    "\n",
    "Want to explore positional encodings next? ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenEmbedding(\n",
       "  (embedding): Embedding(20, 5)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 20  # Suppose our vocab has 20 words\n",
    "d_model = 5      # Each token gets a 5-dimensional embedding\n",
    "\n",
    "embedding_layer = TokenEmbedding(vocab_size, d_model)\n",
    "embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "Output shape: torch.Size([2, 4, 5])\n",
      "\n",
      "Embedded representations:\n",
      " tensor([[[ 0.5315, -0.8051,  0.0987,  0.8236, -1.0337],\n",
      "         [ 0.2961, -1.8903,  0.3494,  0.5855, -0.2111],\n",
      "         [-3.0105,  1.2251, -0.9130,  0.6408, -0.3248],\n",
      "         [-0.0534, -0.8222,  0.1004,  1.3967,  1.2444]],\n",
      "\n",
      "        [[ 2.3621, -0.4427,  1.5551,  2.0475,  0.0160],\n",
      "         [ 0.6351, -2.1685, -0.9456,  1.4489, -0.2879],\n",
      "         [-0.5526,  1.2502, -0.8365, -0.5334,  0.3436],\n",
      "         [ 0.6687, -0.9704, -0.7501, -0.0662, -0.3389]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example tokenized sequences (batch_size=2, seq_len=4)\n",
    "x = torch.tensor([[1, 5, 10, 15], [2, 6, 11, 16]])\n",
    "\n",
    "# Pass through the embedding layer\n",
    "embedded_x = embedding_layer(x)\n",
    "\n",
    "# Print shapes\n",
    "print(f\"Input shape: {x.shape}\")  # (batch_size, seq_len)\n",
    "print(f\"Output shape: {embedded_x.shape}\")  # (batch_size, seq_len, d_model)\n",
    "\n",
    "# View actual embeddings\n",
    "print(\"\\nEmbedded representations:\\n\", embedded_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix (weight parameters):\n",
      "\n",
      "Parameter containing:\n",
      "tensor([[-0.1952, -0.5487, -0.0722,  0.2438, -1.1966],\n",
      "        [ 0.5315, -0.8051,  0.0987,  0.8236, -1.0337],\n",
      "        [ 2.3621, -0.4427,  1.5551,  2.0475,  0.0160],\n",
      "        [ 0.4441, -0.1668, -0.8881,  0.6682, -0.3665],\n",
      "        [-0.1184, -0.8980,  0.8748, -0.0442, -2.3425],\n",
      "        [ 0.2961, -1.8903,  0.3494,  0.5855, -0.2111],\n",
      "        [ 0.6351, -2.1685, -0.9456,  1.4489, -0.2879],\n",
      "        [-0.8877, -1.0203, -0.0877, -0.0692,  0.7135],\n",
      "        [-0.4515, -1.2082,  0.7028,  0.0785,  1.3558],\n",
      "        [-0.0853,  1.0107,  0.4004, -0.9564,  0.7705],\n",
      "        [-3.0105,  1.2251, -0.9130,  0.6408, -0.3248],\n",
      "        [-0.5526,  1.2502, -0.8365, -0.5334,  0.3436],\n",
      "        [-0.3987,  0.0290, -1.8351, -1.4687,  0.2974],\n",
      "        [-0.4602, -0.8119,  0.2518, -0.3838,  0.1372],\n",
      "        [-0.0932,  0.3097, -1.1411,  1.1009, -0.7020],\n",
      "        [-0.0534, -0.8222,  0.1004,  1.3967,  1.2444],\n",
      "        [ 0.6687, -0.9704, -0.7501, -0.0662, -0.3389],\n",
      "        [-2.3487, -1.8488, -1.2706,  0.0355, -0.8823],\n",
      "        [-0.5672, -0.3878, -0.5807,  1.1258,  2.5138],\n",
      "        [ 0.7038, -0.5681, -0.9146, -3.3654,  0.6690]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedding matrix (weight parameters):\\n\")\n",
    "print(embedding_layer.embedding.weight)  # Shows the entire embedding matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Single sequence embedding:\n",
      " tensor([[ 0.4441, -0.1668, -0.8881,  0.6682, -0.3665],\n",
      "        [-0.8877, -1.0203, -0.0877, -0.0692,  0.7135],\n",
      "        [-0.0932,  0.3097, -1.1411,  1.1009, -0.7020]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "\n",
      "Single token embedding:\n",
      " tensor([[-0.1184, -0.8980,  0.8748, -0.0442, -2.3425]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Single sequence\n",
    "single_seq = torch.tensor([3, 7, 14])\n",
    "print(\"\\nSingle sequence embedding:\\n\", embedding_layer(single_seq))\n",
    "\n",
    "# Single token index\n",
    "single_token = torch.tensor([4])\n",
    "print(\"\\nSingle token embedding:\\n\", embedding_layer(single_token))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attention-is-all-you-need",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
